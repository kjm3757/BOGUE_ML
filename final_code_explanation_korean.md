# ğŸ“‹ ìµœì¢… ì•™ìƒë¸” ì½”ë“œ ìƒì„¸ ì„¤ëª…

## ğŸ” ë°ì´í„° ëˆ„ìˆ˜ ê²€í†  ê²°ê³¼

### âœ… **ê²°ë¡ : ë°ì´í„° ëˆ„ìˆ˜ ì—†ìŒ í™•ì¸**

ëª¨ë“  ëª¨ë¸ì—ì„œ train/test ë°ì´í„°ê°€ ì™„ì „íˆ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë©°, test ë°ì´í„°ì˜ ì‹¤ì œ ë§¤ì¶œê°’ì€ ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì „í˜€ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

---

## ğŸ“š ì „ì²´ ì½”ë“œ êµ¬ì¡° ìƒì„¸ ì„¤ëª…

### **PART 0: ì´ˆê¸° ì„¤ì • (ë¼ì¸ 16-114)**

#### **1. Seed ê³ ì • (ì¬í˜„ì„± ë³´ì¥)**
```python
SEED = 42
# Python, NumPy, PyTorch, CUDA ëª¨ë“  seed ê³ ì •
os.environ['PYTHONHASHSEED'] = str(SEED)
torch.backends.cudnn.deterministic = True
```

**ëª©ì **: ê°™ì€ ê²°ê³¼ë¥¼ ì¬í˜„í•˜ê¸° ìœ„í•´ ëª¨ë“  ëœë¤ ìš”ì†Œë¥¼ ê³ ì •

#### **2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**
```python
train = pd.read_csv(TRAIN_PATH)
test = pd.read_csv(TEST_PATH)
acad = pd.read_csv(ACAD_PATH)  # í•™ì‚¬ì¼ì • ë°ì´í„°
```

**í•™ì‚¬ì¼ì • Merge**:
- Trainê³¼ Test ëª¨ë‘ì— í•™ì‚¬ì¼ì • ì •ë³´ ì¶”ê°€
- âœ… **ëˆ„ìˆ˜ ì•„ë‹˜**: í•™ì‚¬ì¼ì •ì€ ë¯¸ë¦¬ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´

**ì¼ë§¤ì¶œ ì •ë¦¬**:
- ì‰¼í‘œ ì œê±°, ìˆ«ì ë³€í™˜
- Test ë°ì´í„°ì˜ ì¼ë§¤ì¶œì€ í‰ê°€ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©

---

### **PART 1: LSTM ëª¨ë¸ (ë¼ì¸ 116-398)**

#### **1.1 Feature Engineering**

**`make_basic_features_lstm(df)`** - ì‹œê°„ ì •ë³´ë§Œ:
- `DayOfWeek`: ìš”ì¼ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
- `Month`, `Day`: ì›”, ì¼
- `IsWeekend`: ì£¼ë§ ì—¬ë¶€
- **`OpHours`**: ì˜ì—… ì‹œê°„ (ì‹œê°„ ë‹¨ìœ„)
  - í•™ê¸° ì¤‘: ì›”-ê¸ˆ 11ì‹œê°„, í† ìš”ì¼ 6ì‹œê°„, ì¼ìš”ì¼ 0ì‹œê°„
  - ë°©í•™ ì¤‘: ì›”-í†  6ì‹œê°„, ì¼ìš”ì¼ 0ì‹œê°„
- `OpHoursFactor`: 0~1ë¡œ ì •ê·œí™” (11ì‹œê°„ ê¸°ì¤€)

**`make_features_lstm(df)`** - ì „ì²´ í”¼ì²˜:
- ê¸°ë³¸ í”¼ì²˜ + ì˜ì—…ì‹œê°„ í”¼ì²˜
- **Lag í”¼ì²˜**: 1, 2, 3, 7, 14, 28ì¼ ì „ ë§¤ì¶œ
- **Rolling í”¼ì²˜**: 
  - `Mean7/14/28`: 7/14/28ì¼ ì´ë™í‰ê· 
  - `Std7/14/28`: 7/14/28ì¼ í‘œì¤€í¸ì°¨
- `IsZeroSales`: íœ´ë¬´ì¼ ì—¬ë¶€ (binary)

**ë°ì´í„° ëˆ„ìˆ˜ ê²€í†  âœ“**:
- âœ… Train ì „ì²´ì—ì„œ feature ìƒì„± í›„ splití•˜ëŠ” ê²ƒì€ ì‹œê³„ì—´ í‘œì¤€
- âœ… Valì˜ lag/rollingì€ train ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ë¯€ë¡œ ë¬¸ì œ ì—†ìŒ

#### **1.2 ë°ì´í„° ì¤€ë¹„**

**Train/Val Split**:
```python
train_df_lstm, val_df_lstm = train_test_split(train_lstm, test_size=0.2, shuffle=False, random_state=SEED)
```
- `shuffle=False`: ì‹œê³„ì—´ì´ë¯€ë¡œ ì‹œê°„ ìˆœì„œ ìœ ì§€

**Feature Selection**:
- `meta_cols_lstm`: Lag/Mean/Std ì œì™¸í•œ ëª¨ë“  numeric í”¼ì²˜
- `tree_features_lstm`: meta_cols + Lag/Mean/Std í”¼ì²˜

**Scaling**:
- `MinMaxScaler`: 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
- Salesì™€ Meta í”¼ì²˜ ê°ê° ë³„ë„ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©

#### **1.3 ëª¨ë¸ êµ¬ì¡°**

**MetaLSTM**:
```
Input: (batch, lookback=21, input_dim)
  â†“
LSTM (2 layers, hidden=128, dropout=0.4)
  â†“
FC1 (128 â†’ 64) + ReLU + Dropout
  â†“
FC2 (64 â†’ 1)
  â†“
Output: (batch, 1)
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Lookback: 21ì¼
- Hidden size: 128
- Layers: 2
- Dropout: 0.4
- Learning rate: 0.0005
- Batch size: 16

#### **1.4 í•™ìŠµ**

**Training Loop**:
1. Forward pass
2. Loss ê³„ì‚° (MSE)
3. Backward pass
4. Gradient clipping (max_norm=1.0)
5. Optimizer step

**Early Stopping**:
- Patience: 15 epochs
- Min delta: 0.0001
- Best model ì €ì¥ ë° ë³µì›

**Learning Rate Scheduling**:
- ReduceLROnPlateau: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ LR ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ

#### **1.5 ì˜ˆì¸¡**

**Autoregressive ì˜ˆì¸¡**:
```python
predict_nn_autoreg_lstm(model, train_df, future_meta_df, lookback)
```

**ê³¼ì •**:
1. Train ë°ì´í„°ì˜ ìµœê·¼ 21ì¼ì„ historyë¡œ ì‚¬ìš©
2. ë‹¤ìŒ ë‚  ì˜ˆì¸¡
3. ì˜ˆì¸¡ê°’ì„ historyì— ì¶”ê°€
4. ë‹¤ìŒ ë‚  ì˜ˆì¸¡ ì‹œ ì´ì „ ì˜ˆì¸¡ê°’ í¬í•¨í•˜ì—¬ ì‚¬ìš©
5. ë°˜ë³µ

**ë°ì´í„° ëˆ„ìˆ˜ ê²€í†  âœ“**:
- âœ… `future_df_lstm`ì—ëŠ” testì˜ ì¼ë§¤ì¶œ ì—†ìŒ (ë‚ ì§œ + í•™ì‚¬ì¼ì •ë§Œ)
- âœ… HistoryëŠ” train ë°ì´í„°ë§Œ ì‚¬ìš©
- âœ… ì˜ˆì¸¡ê°’ë§Œ ë‹¤ìŒ stepì— ì‚¬ìš©

**Post-processing**:
```python
postprocess_zero_days_lstm(preds, future_dates, train_df, threshold_ratio=0.7, small_pred_threshold=10000)
```

**ê·œì¹™**:
- Train ë°ì´í„°ì—ì„œ ì›”-ì¼ë³„ë¡œ 0ì¸ ë¹„ìœ¨ ê³„ì‚°
- 70% ì´ìƒ 0ì¸ ë‚ ì§œ + ì˜ˆì¸¡ê°’ < 10,000 â†’ 0ìœ¼ë¡œ ê°•ì œ

---

### **PART 2: GRU ëª¨ë¸ (ë¼ì¸ 400-765)**

#### **2.1 Feature Engineering**

**`make_features_gru(df)`**:
- LSTMê³¼ ìœ ì‚¬ + ì¶”ê°€ í”¼ì²˜:
  - `Mean3`: 3ì¼ ì´ë™í‰ê· 
  - `Max7`, `Min7`: 7ì¼ ìµœëŒ€/ìµœì†Œê°’
  - `CV7`: ë³€ë™ê³„ìˆ˜ (Std7 / Mean7)
  - `MonthAvg`: ì›”ë³„ í‰ê·  ë§¤ì¶œ
  - `WeekdayAvg`: ìš”ì¼ë³„ í‰ê·  ë§¤ì¶œ

**`make_features_gru_safe(df, month_avg_train, weekday_avg_train)`**:
- âœ… **ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ë²„ì „**
- Trainì˜ í‰ê· ê°’ì„ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì•„ ì‚¬ìš©
- Test ë°ì´í„°ì—ì„œ ì§ì ‘ í‰ê·  ê³„ì‚° ì•ˆ í•¨

#### **2.2 ëª¨ë¸ êµ¬ì¡°**

**MetaGRU**:
```
Input: (batch, lookback=7, input_dim)
  â†“
GRU (2 layers, hidden=64, dropout=0.3)
  â†“
FC1 (64 â†’ 32) + ReLU + Dropout
  â†“
FC2 (32 â†’ 1)
  â†“
Output: (batch, 1)
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Lookback: 7ì¼ (LSTMë³´ë‹¤ ì§§ìŒ)
- Hidden size: 64
- Layers: 2
- Dropout: 0.3
- Learning rate: 0.0001
- Batch size: 64

#### **2.3 ì˜ˆì¸¡ (Direct Multi-step)**

**`predict_gru_direct_safe`**:
- âœ… Trainì˜ í‰ê· ê°’ ì‚¬ì „ ê³„ì‚°:
  ```python
  month_avg_train_gru = train_full_gru_for_avg.groupby("Month")["ì¼ë§¤ì¶œ"].mean()
  weekday_avg_train_gru = train_full_gru_for_avg.groupby("DayOfWeek")["ì¼ë§¤ì¶œ"].mean()
  ```

**ì˜ˆì¸¡ ê³¼ì •**:
1. Train ë°ì´í„°ë¥¼ historyë¡œ ì´ˆê¸°í™”
2. ê° stepë§ˆë‹¤:
   - í•„ìš”í•˜ë©´ ì´ì „ ì˜ˆì¸¡ê°’ìœ¼ë¡œ sequence êµ¬ì„±
   - **Trainì˜ í‰ê· ê°’ë§Œ ì‚¬ìš©**í•˜ì—¬ MonthAvg, WeekdayAvg ìƒì„±
   - 7ì¼ sequenceë¡œ ì˜ˆì¸¡
   - ì˜ˆì¸¡ê°’ì„ historyì— ì¶”ê°€
3. ë°˜ë³µ

**ë°ì´í„° ëˆ„ìˆ˜ ê²€í†  âœ“**:
- âœ… `future_df_gru`ëŠ” ë‚ ì§œ + í•™ì‚¬ì¼ì •ë§Œ (ì¼ë§¤ì¶œ ì—†ìŒ)
- âœ… Trainì˜ í‰ê· ê°’ë§Œ ì‚¬ìš© (test ë°ì´í„° ì‚¬ìš© ì•ˆ í•¨)
- âœ… HistoryëŠ” train + ì˜ˆì¸¡ê°’ë§Œ ì‚¬ìš©

---

### **PART 3: Tree ëª¨ë¸ (ë¼ì¸ 767-1006)**

#### **3.1 Feature Engineering**

**`make_features_tree(df)`**:
- GRUì™€ ë™ì¼í•œ í”¼ì²˜ ìƒì„±
- MonthAvg, WeekdayAvg í¬í•¨

#### **3.2 ëª¨ë¸ í•™ìŠµ**

**LightGBM**:
- Objective: regression (RMSE)
- num_leaves: 100
- learning_rate: 0.01
- max_depth: 10
- Regularization: alpha=0.1, lambda=2.0

**XGBoost**:
- Objective: reg:squarederror (RMSE)
- max_depth: 5
- learning_rate: 0.01
- Regularization: alpha=0.1, lambda=2.0

**ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ**:
- Validationì—ì„œ ì°¾ì€ best iterationìœ¼ë¡œ ì „ì²´ train ë°ì´í„°ì— ì¬í•™ìŠµ
- âœ… Test ë°ì´í„° ì‚¬ìš© ì•ˆ í•¨

#### **3.3 ì˜ˆì¸¡ (Autoregressive)**

**`predict_tree_autoreg`**:
- `history_df`ë¡œ `train_full_tree`ë§Œ ì „ë‹¬
- ê° stepë§ˆë‹¤:
  1. í˜„ì¬ê¹Œì§€ì˜ historyë¡œ feature ìƒì„±
  2. ëª¨ë¸ ì˜ˆì¸¡
  3. **Scale ë³´ì •**:
     - ë„ˆë¬´ ì‘ì€ ì˜ˆì¸¡ê°’ ë³´ì • (recent_7d_avg ê¸°ì¤€)
     - ìµœì†Œê°’ ë³´ì¥ (10,000)
  4. ì˜ˆì¸¡ê°’ì„ historyì— ì¶”ê°€í•˜ê³  feature ì¬ìƒì„±
  5. ë°˜ë³µ

**ë°ì´í„° ëˆ„ìˆ˜ ê²€í†  âœ“**:
- âœ… Test ë°ì´í„° ì „í˜€ ì‚¬ìš© ì•ˆ í•¨
- âœ… `train_full_tree`ë§Œ ì‚¬ìš©
- âœ… ì˜ˆì¸¡ê°’ìœ¼ë¡œë§Œ ë‹¤ìŒ step ì§„í–‰

---

### **PART 4: ìµœì¢… ì•™ìƒë¸” (ë¼ì¸ 1008-1035)**

#### **ì•™ìƒë¸” êµ¬ì¡°**:

**1ë‹¨ê³„: NN ì•™ìƒë¸”**
```python
future_nn_ensemble = (future_lstm + future_gru) / 2
```
- LSTMê³¼ GRUì˜ ë‹¨ìˆœ í‰ê· 

**2ë‹¨ê³„: 1ì°¨ ìµœì¢… ì•™ìƒë¸”**
```python
future_final = 0.3 * future_tree + 0.7 * future_nn_ensemble
```
- Tree ì•™ìƒë¸”: 30%
- NN ì•™ìƒë¸”: 70%

**3ë‹¨ê³„: 2ì°¨ ìµœì¢… ì•™ìƒë¸”**
```python
future_final2 = 0.6 * future_nn_ensemble + 0.4 * future_final
```
- NN ì•™ìƒë¸”: 60%
- 1ì°¨ ìµœì¢… ì•™ìƒë¸”: 40%

**ìµœì¢… ê°€ì¤‘ì¹˜ ë¶„í•´**:
- Tree: 0.3 Ã— 0.4 = 0.12 (12%)
- NN: 0.7 Ã— 0.4 + 0.6 = 0.88 (88%)

---

### **PART 5: ì„±ëŠ¥ í‰ê°€ ë° ì €ì¥ (ë¼ì¸ 1037-1126)**

#### **Metrics**:
- **MAE** (Mean Absolute Error): í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
- **RMSE** (Root Mean Squared Error): í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨
- **SMAPE** (Symmetric Mean Absolute Percentage Error): ëŒ€ì¹­ í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨

#### **ê²°ê³¼ ì €ì¥**:
- ëª¨ë“  ëª¨ë¸ ì˜ˆì¸¡ê°’ + ìµœì¢… ì•™ìƒë¸”ì„ CSVë¡œ ì €ì¥
- `final_test_prediction_optimal.csv`

---

## ğŸ”‘ í•µì‹¬ íŠ¹ì§• ìš”ì•½

### **1. ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€**
- âœ… Train/Test ì™„ì „ ë¶„ë¦¬
- âœ… Trainì˜ í†µê³„ê°’ë§Œ ì‚¬ìš© (MonthAvg, WeekdayAvg)
- âœ… Test ë°ì´í„°ì˜ ì‹¤ì œ ë§¤ì¶œê°’ ì‚¬ìš© ì•ˆ í•¨
- âœ… ì˜ˆì¸¡ê°’ìœ¼ë¡œë§Œ ë‹¤ìŒ step ì§„í–‰

### **2. ì˜ì—…ì‹œê°„ í”¼ì²˜**
- í•™ê¸°/ë°©í•™ êµ¬ë¶„ì— ë”°ë¥¸ ì‹¤ì œ ì˜ì—…ì‹œê°„ ë°˜ì˜
- í•™ê¸° ì¤‘: ì›”-ê¸ˆ 11ì‹œê°„, í† ìš”ì¼ 6ì‹œê°„
- ë°©í•™ ì¤‘: ì›”-í†  6ì‹œê°„
- ì¼ìš”ì¼: í•­ìƒ 0ì‹œê°„

### **3. ëª¨ë¸ ë‹¤ì–‘ì„±**
- **LSTM**: 21ì¼ lookback, autoregressive
- **GRU**: 7ì¼ lookback, direct multi-step
- **LightGBM/XGBoost**: Tree ê¸°ë°˜, autoregressive

### **4. ë‹¤ì¸µ ì•™ìƒë¸”**
- 1ë‹¨ê³„: NN ì•™ìƒë¸” (LSTM + GRU)
- 2ë‹¨ê³„: 1ì°¨ ìµœì¢… (Tree + NN)
- 3ë‹¨ê³„: 2ì°¨ ìµœì¢… (NN + 1ì°¨ ìµœì¢…)

### **5. Post-processing**
- Zero-sales day íŒ¨í„´ í•™ìŠµ ë° ì ìš©
- Scale ë³´ì • (Tree ëª¨ë¸)
- ìµœì†Œê°’ ë³´ì¥

---

## âœ… ìµœì¢… ê²€ì¦

**ë°ì´í„° ëˆ„ìˆ˜ ì—†ìŒ í™•ì¸**:
1. âœ… LSTM: Test ì¼ë§¤ì¶œ ì‚¬ìš© ì•ˆ í•¨
2. âœ… GRU: Train í‰ê· ê°’ë§Œ ì‚¬ìš©
3. âœ… Tree: Test ë°ì´í„° ì „í˜€ ì‚¬ìš© ì•ˆ í•¨
4. âœ… í•™ì‚¬ì¼ì •: ë¯¸ë¦¬ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´ (ëˆ„ìˆ˜ ì•„ë‹˜)

**ì½”ë“œ í’ˆì§ˆ**:
- ì¬í˜„ì„± ë³´ì¥ (Seed ê³ ì •)
- ëª…í™•í•œ êµ¬ì¡° (PART 1/2/3 ë¶„ë¦¬)
- ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ë¡œì§ ëª…ì‹œ

