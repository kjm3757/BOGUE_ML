# -*- coding: utf-8 -*-
"""XGB_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Cjd7ePuEm29nIE4kyInMWGAd8ZNqIIV
"""

import pandas as pd
import numpy as np
import random
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import mean_absolute_error, mean_squared_error
import xgboost as xgb

# =========================
# ⭐ 전역 시드 고정 (재현성 보장)
# =========================
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

FEATURE_PATH = "/content/drive/MyDrive/기계학습/팀플/Data/학사일정_정리(2325).csv"
TRAIN_PATH   = "/content/drive/MyDrive/기계학습/팀플/Data/ENG_POS_train_val.csv"
TEST_PATH = "/content/drive/MyDrive/기계학습/팀플/Data/ENG_POS_test.csv"

# =========================
# 1. 유틸 함수들
# =========================
def smape(y_true, y_pred):
    y_true = np.asarray(y_true, float)
    y_pred = np.asarray(y_pred, float)
    return np.mean(
        2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    ) * 100


def calculate_operating_hours(row):
    weekday = row["weekday"]
    semester = row["semester"]
    holiday = row["holiday"]

    if weekday == "sun":
        return 0

    if semester == 1:
        if holiday == 1:
            return 7
        if weekday in ["mon", "tue", "wed", "thu", "fri"]:
            return 12
        if weekday == "sat":
            return 7
        return 0

    else:
        if holiday == 1:
            return 0
        if weekday in ["mon", "tue", "wed", "thu", "fri", "sat"]:
            return 7
        return 0


# =========================
# 2. 메인 파이프라인
# =========================
def run_xgb(feature_path=FEATURE_PATH, train_path=TRAIN_PATH, test_path=TEST_PATH):

    # ---------- (1) 데이터 로드 ----------
    feat = pd.read_csv(feature_path)
    pos_train = pd.read_csv(train_path)
    pos_test = pd.read_csv(test_path)

    for df in [pos_train, pos_test]:
        for col in ["daily", "AOV"]:
            df[col] = (
                df[col].astype(str).str.replace(",", "", regex=False).astype(float)
            )
        df["date"] = pd.to_datetime(df["date"])

    feat["date"] = pd.to_datetime(feat["date"])

    pos_train["set"] = "train"
    pos_test["set"] = "test"

    # 전체 병합 (Static Feature 계산용)
    pos_all = pd.concat([pos_train, pos_test], ignore_index=True)
    df_all = pd.merge(pos_all, feat, on="date", how="left")
    df_all = df_all.sort_values("date").reset_index(drop=True)

    print("All merged shape:", df_all.shape)

    # ---------- (2) Static Feature Engineering ----------
    df_all["operating_hours"] = df_all.apply(calculate_operating_hours, axis=1)

    # [New Feature] Month (계절성 반영)
    df_all["month"] = df_all["date"].dt.month

    df_all["exam_before3"] = df_all["exam"].shift(1).rolling(3, min_periods=1).sum().fillna(0)
    df_all["exam_after3"] = df_all["exam"].shift(-1).rolling(3, min_periods=1).sum().fillna(0)

    df_all["semester_weekend"] = df_all["semester"] * df_all["weekend"]

    df_all = pd.get_dummies(df_all, columns=["weekday"], drop_first=True)

    # ---------- (3) Train/Test 분리 ----------
    df_train_val = df_all[df_all["set"] == "train"].copy()
    df_test_static = df_all[df_all["set"] == "test"].copy() # daily 값은 평가용으로만 씀

    # [Train Phase] 학습 데이터에는 실제 daily 값을 이용하여 Lag 생성
    print("\nGenerating features for Training set...")
    lag_list = [1, 2, 3, 7, 14, 28]
    win_list = [7, 14, 28]

    for lag in lag_list:
        df_train_val[f"Lag{lag}"] = df_train_val["daily"].shift(lag)

    for win in win_list:
        roll = df_train_val["daily"].rolling(window=win, min_periods=1)
        df_train_val[f"RollingMean{win}"] = roll.mean().shift(1)
        df_train_val[f"RollingStd{win}"] = roll.std(ddof=0).shift(1)

    # NaN 제거
    feature_na_cols = [f"Lag{l}" for l in lag_list] + \
                      [f"RollingMean{w}" for w in win_list] + \
                      [f"RollingStd{w}" for w in win_list]

    df_train_val_clean = df_train_val.dropna(subset=feature_na_cols).reset_index(drop=True)
    df_train_val_clean = df_train_val_clean[df_train_val_clean["daily"] > 0].copy()

    print("Train_val prepared:", df_train_val_clean.shape)

    # ---------- (4) Train/Val Split & Model Training (Log Transform) ----------
    df_sorted = df_train_val_clean.sort_values("date").reset_index(drop=True)
    split_idx = int(len(df_sorted) * 0.8)

    train = df_sorted.iloc[:split_idx]
    val = df_sorted.iloc[split_idx:]

    drop_cols = ["date", "daily", "num", "AOV", "set"]
    feature_cols = [c for c in df_sorted.columns if c not in drop_cols]

    X_train = train[feature_cols]
    y_train = np.log1p(train["daily"]) # Log Transform
    X_val = val[feature_cols]
    y_val = np.log1p(val["daily"])     # Log Transform

    print(f"Training with {len(feature_cols)} features (Target Log-Transformed)...")

    from itertools import product

    param_grid = {
        "max_depth":        [4, 5, 6],
        "learning_rate":    [0.03, 0.05],
        "n_estimators":     [400, 800],
        "min_child_weight": [2, 4],
        "subsample":        [0.7, 0.9],
        "colsample_bytree": [0.7, 0.9],
    }

    best_rmse = np.inf
    best_params = None

    print("\n===== Hyperparameter Search (based on Val RMSE, XGBoost) =====")

    for max_depth, lr, n_estimators, min_child_weight, subsample, colsample in product(
        param_grid["max_depth"],
        param_grid["learning_rate"],
        param_grid["n_estimators"],
        param_grid["min_child_weight"],
        param_grid["subsample"],
        param_grid["colsample_bytree"],
    ):
        params = {
            "objective": "reg:squarederror",
            "max_depth": max_depth,
            "learning_rate": lr,
            "n_estimators": n_estimators,
            "min_child_weight": min_child_weight,
            "subsample": subsample,
            "colsample_bytree": colsample,
            "random_state": SEED,
            "n_jobs": -1,
            "tree_method": "hist",
            "sampling_method": "uniform",
            "enable_categorical": False,
        }

        reg_tmp = xgb.XGBRegressor(**params)

        reg_tmp.fit(X_train, y_train)
        val_pred_tmp = reg_tmp.predict(X_val)
        rmse_tmp = np.sqrt(mean_squared_error(y_val, val_pred_tmp))

        if rmse_tmp < best_rmse:
            best_rmse = rmse_tmp
            best_params = params

    print("\n>>> Best Params (by Val RMSE):")
    print(best_params)
    print(f">>> Best Val RMSE: {best_rmse:,.4f}")

    # Final Fit
    reg = xgb.XGBRegressor(**best_params)
    reg.fit(df_sorted[feature_cols], np.log1p(df_sorted["daily"]))

    # ---------- (5) Recursive Forecasting for Test (Black Box Simulation) ----------
    print("\nStarting Recursive Forecasting for Test Set...")

    history_df = df_train_val.iloc[-60:].copy()
    test_preds = []

    for idx, row in df_test_static.iterrows():
        # 1. 현재 예측할 날짜의 기본 정보 가져오기
        current_row = row.to_frame().T
        current_row["daily"] = np.nan

        # 2. History에 임시 추가
        temp_history = pd.concat([history_df, current_row], ignore_index=True)

        # 3. Feature Engineering (Lag/Rolling)
        for lag in lag_list:
            temp_history[f"Lag{lag}"] = temp_history["daily"].shift(lag)

        for win in win_list:
            roll = temp_history["daily"].rolling(window=win, min_periods=1)
            temp_history[f"RollingMean{win}"] = roll.mean().shift(1)
            temp_history[f"RollingStd{win}"] = roll.std(ddof=0).shift(1)

        # 4. 예측 수행
        X_test_single = temp_history.iloc[[-1]][feature_cols]

        # [Bug Fix] Object -> Float 변환
        X_test_single = X_test_single.astype(float)

        # Predict Log Value
        pred_log = reg.predict(X_test_single)[0]

        # Inverse Transform (Exp)
        pred = np.expm1(pred_log)

        test_preds.append(pred)

        # 5. History 업데이트 (예측값을 daily로 확정)
        current_row["daily"] = pred
        history_df = pd.concat([history_df, current_row], ignore_index=True).iloc[-60:]

    # ---------- (6) 성능 평가 ----------
    y_test_actual = df_test_static["daily"].values

    mask = y_test_actual > 0
    final_actual = y_test_actual[mask]
    final_pred = np.array(test_preds)[mask]

    mae_test = mean_absolute_error(final_actual, final_pred)
    rmse_test = np.sqrt(mean_squared_error(final_actual, final_pred))
    smape_test = smape(final_actual, final_pred)

    print("\n===== Test Performance (Recursive / Black-box / XGBoost) =====")
    print(f"MAE   : {mae_test:,.2f}")
    print(f"RMSE  : {rmse_test:,.2f}")
    print(f"SMAPE : {smape_test:.2f}%")

    result_df = df_test_static.copy()
    result_df["pred_daily"] = test_preds
    result_df = result_df[result_df["daily"] > 0]

    result_df = result_df[["date", "daily", "pred_daily"]].rename(columns={"daily": "actual_daily"})
    result_df.to_csv("xgb_prediction_recursive.csv", index=False, encoding="utf-8-sig")

    metrics = {
        "model": "XGB_Recursive",
        "test_MAE": mae_test,
        "test_RMSE": rmse_test,
        "test_SMAPE": smape_test,
    }

    return metrics, result_df


if __name__ == "__main__":
    metrics, df_pred = run_xgb()
    print("\n[DEBUG] XGB metrics:", metrics)
    print(df_pred.head())