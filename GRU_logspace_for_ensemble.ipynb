{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sympy==1.12\n",
        "import sympy\n",
        "print(sympy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "VRKp0rG--yup",
        "outputId": "35a35f4e-0f85-4901-a872-d6ecc5e0c206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/5.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m198.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0+cu126 requires sympy>=1.13.3, but you have sympy 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "23d890d3785a4ec2bec07e971f2c137c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Z991GZ8nYS",
        "outputId": "4829603f-b4e9-436f-8dd2-a2992bf6a65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] Loss: 5.1248\n",
            "[Epoch 2/50] Loss: 3.4537\n",
            "[Epoch 3/50] Loss: 3.0589\n",
            "[Epoch 4/50] Loss: 2.8459\n",
            "[Epoch 5/50] Loss: 2.5098\n",
            "[Epoch 6/50] Loss: 2.3306\n",
            "[Epoch 7/50] Loss: 2.2322\n",
            "[Epoch 8/50] Loss: 2.0843\n",
            "[Epoch 9/50] Loss: 1.7841\n",
            "[Epoch 10/50] Loss: 1.6079\n",
            "[Epoch 11/50] Loss: 1.5385\n",
            "[Epoch 12/50] Loss: 1.4832\n",
            "[Epoch 13/50] Loss: 1.4270\n",
            "[Epoch 14/50] Loss: 1.4800\n",
            "[Epoch 15/50] Loss: 1.4239\n",
            "[Epoch 16/50] Loss: 1.5282\n",
            "[Epoch 17/50] Loss: 1.4763\n",
            "[Epoch 18/50] Loss: 1.4326\n",
            "[Epoch 19/50] Loss: 1.4526\n",
            "[Epoch 20/50] Loss: 1.3838\n",
            "[Epoch 21/50] Loss: 1.3701\n",
            "[Epoch 22/50] Loss: 1.4209\n",
            "[Epoch 23/50] Loss: 1.3956\n",
            "[Epoch 24/50] Loss: 1.4222\n",
            "[Epoch 25/50] Loss: 1.3865\n",
            "[Epoch 26/50] Loss: 1.3789\n",
            "[Epoch 27/50] Loss: 1.3619\n",
            "[Epoch 28/50] Loss: 1.3619\n",
            "[Epoch 29/50] Loss: 1.4993\n",
            "[Epoch 30/50] Loss: 1.3531\n",
            "[Epoch 31/50] Loss: 1.3833\n",
            "[Epoch 32/50] Loss: 1.3628\n",
            "[Epoch 33/50] Loss: 1.4028\n",
            "[Epoch 34/50] Loss: 1.3772\n",
            "[Epoch 35/50] Loss: 1.3588\n",
            "[Epoch 36/50] Loss: 1.4113\n",
            "[Epoch 37/50] Loss: 1.2954\n",
            "[Epoch 38/50] Loss: 1.3423\n",
            "[Epoch 39/50] Loss: 1.3262\n",
            "[Epoch 40/50] Loss: 1.3381\n",
            "[Epoch 41/50] Loss: 1.3365\n",
            "[Epoch 42/50] Loss: 1.3261\n",
            "[Epoch 43/50] Loss: 1.3397\n",
            "[Epoch 44/50] Loss: 1.3068\n",
            "[Epoch 45/50] Loss: 1.2775\n",
            "[Epoch 46/50] Loss: 1.3095\n",
            "[Epoch 47/50] Loss: 1.2689\n",
            "[Epoch 48/50] Loss: 1.3132\n",
            "[Epoch 49/50] Loss: 1.3532\n",
            "[Epoch 50/50] Loss: 1.3176\n",
            "\n",
            "====== ìµœì¢… ì„±ëŠ¥ (Optimized GRU / log-space) ======\n",
            "MAE : 222036.22427264767\n",
            "RMSE: 578897.0934297956\n",
            "SMAPE: 157.80741392791214\n",
            "         ì˜ì—…ì¼ì          ì˜ˆì¸¡ë§¤ì¶œ\n",
            "0  2025-08-10      1.702833\n",
            "1  2025-08-11    602.023135\n",
            "2  2025-08-12   3313.961170\n",
            "3  2025-08-13   8506.853344\n",
            "4  2025-08-14   3772.921769\n",
            "..        ...           ...\n",
            "78 2025-10-27   8311.216430\n",
            "79 2025-10-28  41805.864722\n",
            "80 2025-10-29  34681.840806\n",
            "81 2025-10-30  29936.042643\n",
            "82 2025-10-31  13821.039384\n",
            "\n",
            "[83 rows x 2 columns]\n",
            "\n",
            "ğŸ“Œ ì €ì¥ ì™„ë£Œ â†’ prediction_gru_logspace.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed ê³ ì •\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) í‰ê°€ ì§€í‘œ\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ì¼ë§¤ì¶œ'] = (\n",
        "        df['ì¼ë§¤ì¶œ']\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ì¼ë§¤ì¶œ'] = pd.to_numeric(df['ì¼ë§¤ì¶œ'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ì˜ì—…ì¼ì'].dt.month\n",
        "    df['day'] = df['ì˜ì—…ì¼ì'].dt.day\n",
        "    df['weekday'] = df['ì˜ì—…ì¼ì'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ì¼ë§¤ì¶œ'].shift(1)\n",
        "    df['lag7'] = df['ì¼ë§¤ì¶œ'].shift(7)\n",
        "    df['lag14'] = df['ì¼ë§¤ì¶œ'].shift(14)\n",
        "    df['lag28'] = df['ì¼ë§¤ì¶œ'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ì¼ë§¤ì¶œ'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ì¼ë§¤ì¶œ'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ì¼ë§¤ì¶œ'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ì¼ë§¤ì¶œ'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ì¼ë§¤ì¶œ'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# Sliding Window\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ì¼ë§¤ì¶œ_log'].values   # â† log-space\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================================================\n",
        "# â­â­ ìµœì í™”ëœ GRU ëª¨ë¸ (log-space í•™ìŠµ) â­â­\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])   # log-space ì¶œë ¥\n",
        "\n",
        "# =========================================================\n",
        "# Autoregressive ì˜ˆì¸¡ (log-space)\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, df_train, df_test, seq_len, feature_cols, scaler_X, scaler_y):\n",
        "\n",
        "    history = df_train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        row = df_test.iloc[i].copy()\n",
        "\n",
        "        # ìµœê·¼ raw ë§¤ì¶œ (log-space â†’ real-space)\n",
        "        last_real = np.expm1(history['ì¼ë§¤ì¶œ_log'].values)\n",
        "\n",
        "        # ---- lag ì—…ë°ì´íŠ¸ ----\n",
        "        row['lag1']  = last_real[-1]\n",
        "        row['lag7']  = last_real[-7]\n",
        "        row['lag14'] = last_real[-14]\n",
        "        row['lag28'] = last_real[-28]\n",
        "\n",
        "        # rolling\n",
        "        row['roll_mean7']  = pd.Series(last_real[-7:]).mean()\n",
        "        row['roll_mean14'] = pd.Series(last_real[-14:]).mean()\n",
        "        row['roll_mean28'] = pd.Series(last_real[-28:]).mean()\n",
        "\n",
        "        row['roll_std7']   = pd.Series(last_real[-7:]).std()\n",
        "        row['roll_std28']  = pd.Series(last_real[-28:]).std()\n",
        "\n",
        "        # ì‹œí€€ìŠ¤ êµ¬ì„± (feature scaling)\n",
        "        seq_df = history.tail(seq_len).copy()\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # -------- GRU log ì˜ˆì¸¡ --------\n",
        "        pred_log_scaled = model(X).item()\n",
        "        pred_log = scaler_y.inverse_transform([[pred_log_scaled]])[0][0]\n",
        "\n",
        "        # -------- log â†’ real-space ë³µì› --------\n",
        "        pred_real = np.expm1(pred_log)\n",
        "        pred_real = max(pred_real, 0)\n",
        "\n",
        "        preds.append(pred_real)\n",
        "\n",
        "        # -------- history ì—…ë°ì´íŠ¸: log-spaceë¡œ ì €ì¥ --------\n",
        "        row['ì¼ë§¤ì¶œ_log'] = pred_log\n",
        "        history = pd.concat([history, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# =========================================================\n",
        "# ğŸ”¥ ë©”ì¸ ì‹¤í–‰\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/ê¸°ê³„í•™ìŠµ/íŒ€í”Œ/Data/POS_train_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/ê¸°ê³„í•™ìŠµ/íŒ€í”Œ/Data/POS_test.csv')\n",
        "\n",
        "train['ì˜ì—…ì¼ì'] = pd.to_datetime(train['ì˜ì—…ì¼ì'])\n",
        "test['ì˜ì—…ì¼ì']  = pd.to_datetime(test['ì˜ì—…ì¼ì'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test  = clean_sales(test)\n",
        "\n",
        "# í•™ì‚¬ì¼ì • merge\n",
        "academic = pd.read_csv('/content/drive/MyDrive/ê¸°ê³„í•™ìŠµ/íŒ€í”Œ/Data/í•™ì‚¬ì¼ì •_ì •ë¦¬(2325).csv')\n",
        "academic['ì˜ì—…ì¼ì'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ì˜ì—…ì¼ì', how='left')\n",
        "test  = test.merge(academic, on='ì˜ì—…ì¼ì',  how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test  = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test  = add_lag_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# Feature 15ê°œ (GRU ì „ìš©)\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend', 'acad_semester', 'acad_weekday',\n",
        "    'open_hours', 'acad_ceremony', 'acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "# =========================================================\n",
        "# log-space íƒ€ê¹ƒ ìƒì„±\n",
        "# =========================================================\n",
        "train['ì¼ë§¤ì¶œ_log'] = np.log1p(train['ì¼ë§¤ì¶œ'])\n",
        "test['ì¼ë§¤ì¶œ_log']  = np.log1p(test['ì¼ë§¤ì¶œ'])\n",
        "\n",
        "# =========================================================\n",
        "# Scaling (feature + log-target)\n",
        "# =========================================================\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "train[['ì¼ë§¤ì¶œ_log']] = scaler_y.fit_transform(train[['ì¼ë§¤ì¶œ_log']])\n",
        "\n",
        "# =========================================================\n",
        "# GRU í•™ìŠµ\n",
        "# =========================================================\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/50] Loss: {total_loss:.4f}\")\n",
        "\n",
        "# =========================================================\n",
        "# Test ì˜ˆì¸¡ (log-space â†’ real)\n",
        "# =========================================================\n",
        "test_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X, scaler_y)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ì¼ë§¤ì¶œ'], test_pred)\n",
        "\n",
        "print(\"\\n====== ìµœì¢… ì„±ëŠ¥ (Optimized GRU / log-space) ======\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "\n",
        "test['ì˜ˆì¸¡ë§¤ì¶œ'] = test_pred\n",
        "print(test[['ì˜ì—…ì¼ì', 'ì˜ˆì¸¡ë§¤ì¶œ']])\n",
        "\n",
        "test[['ì˜ì—…ì¼ì', 'ì˜ˆì¸¡ë§¤ì¶œ']].to_csv(\"prediction_gru_logspace.csv\", index=False)\n",
        "\n",
        "print(\"\\nğŸ“Œ ì €ì¥ ì™„ë£Œ â†’ prediction_gru_logspace.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì„±ëŠ¥ ì•ˆì •í™”**"
      ],
      "metadata": {
        "id": "VWfrWu9D_pjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed ê³ ì •\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) í‰ê°€ ì§€í‘œ\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ì¼ë§¤ì¶œ'] = (\n",
        "        df['ì¼ë§¤ì¶œ']\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ì¼ë§¤ì¶œ'] = pd.to_numeric(df['ì¼ë§¤ì¶œ'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ì˜ì—…ì¼ì'].dt.month\n",
        "    df['day'] = df['ì˜ì—…ì¼ì'].dt.day\n",
        "    df['weekday'] = df['ì˜ì—…ì¼ì'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ì¼ë§¤ì¶œ'].shift(1)\n",
        "    df['lag7'] = df['ì¼ë§¤ì¶œ'].shift(7)\n",
        "    df['lag14'] = df['ì¼ë§¤ì¶œ'].shift(14)\n",
        "    df['lag28'] = df['ì¼ë§¤ì¶œ'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ì¼ë§¤ì¶œ'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ì¼ë§¤ì¶œ'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ì¼ë§¤ì¶œ'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ì¼ë§¤ì¶œ'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ì¼ë§¤ì¶œ'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# Sliding Window\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ì¼ë§¤ì¶œ_log'].values   # log-space target\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================================================\n",
        "# â­â­ ì•ˆì • ë²„ì „ GRU ëª¨ë¸ â­â­\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=96, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])   # log-space ì¶œë ¥\n",
        "\n",
        "# =========================================================\n",
        "# Autoregressive ì˜ˆì¸¡ (log-space ì•ˆì • ë²„ì „)\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, train_df, test_df, seq_len, feature_cols, scaler_X):\n",
        "\n",
        "    # ì´ˆê¸° historyëŠ” trainì—ì„œ â€œseq_len ì´í›„ êµ¬ê°„â€ ì‚¬ìš© â†’ lag/rolling 100% ì •ìƒ\n",
        "    history_real = train_df['ì¼ë§¤ì¶œ'].tolist()\n",
        "    history_log  = train_df['ì¼ë§¤ì¶œ_log'].tolist()\n",
        "\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "        row = test_df.iloc[i].copy()\n",
        "\n",
        "        # í˜„ì¬ê¹Œì§€ real-history ë°°ì—´\n",
        "        arr = np.array(history_real)\n",
        "\n",
        "        # -------- lag ê³„ì‚° (ì•ˆì „ ëª¨ë“œ) --------\n",
        "        def safe_lag(k):\n",
        "            return arr[-k] if len(arr) >= k else arr[0]\n",
        "\n",
        "        row['lag1']  = safe_lag(1)\n",
        "        row['lag7']  = safe_lag(7)\n",
        "        row['lag14'] = safe_lag(14)\n",
        "        row['lag28'] = safe_lag(28)\n",
        "\n",
        "        # -------- rolling --------\n",
        "        def safe_roll(k):\n",
        "            if len(arr) >= k:\n",
        "                return np.mean(arr[-k:])\n",
        "            else:\n",
        "                return np.mean(arr)\n",
        "\n",
        "        def safe_std(k):\n",
        "            if len(arr) >= k:\n",
        "                return np.std(arr[-k:])\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "        row['roll_mean7']  = safe_roll(7)\n",
        "        row['roll_mean14'] = safe_roll(14)\n",
        "        row['roll_mean28'] = safe_roll(28)\n",
        "\n",
        "        row['roll_std7']   = safe_std(7)\n",
        "        row['roll_std28']  = safe_std(28)\n",
        "\n",
        "        # ===== seq_df ìƒì„± =====\n",
        "        # í•­ìƒ train_df ê¸°ë°˜ (sequence ì•ˆì •)\n",
        "        seq_df = train_df.tail(seq_len).copy()\n",
        "\n",
        "        # feature scaling\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # -------- GRU log ì˜ˆì¸¡ --------\n",
        "        pred_log = model(X).item()\n",
        "\n",
        "        # ì•ˆì •í™”\n",
        "        pred_log = np.clip(pred_log, -3, 12)\n",
        "\n",
        "        # log â†’ real\n",
        "        pred_real = np.expm1(pred_log)\n",
        "        pred_real = max(pred_real, 0)\n",
        "\n",
        "        preds.append(pred_real)\n",
        "\n",
        "        # history ì—…ë°ì´íŠ¸\n",
        "        history_real.append(pred_real)\n",
        "        history_log.append(pred_log)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# ğŸ”¥ ë©”ì¸ ì‹¤í–‰\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/ê¸°ê³„í•™ìŠµ/íŒ€í”Œ/Data/POS_train_val.csv')\n",
        "test  = pd.read_csv('/content/drive/MyDrive/ê¸°ê³„í•™ìŠµ/íŒ€í”Œ/Data/POS_test.csv')\n",
        "\n",
        "train['ì˜ì—…ì¼ì'] = pd.to_datetime(train['ì˜ì—…ì¼ì'])\n",
        "test['ì˜ì—…ì¼ì']  = pd.to_datetime(test['ì˜ì—…ì¼ì'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test  = clean_sales(test)\n",
        "\n",
        "# í•™ì‚¬ì¼ì • merge\n",
        "academic = pd.read_csv('/content/drive/MyDrive/ê¸°ê³„í•™ìŠµ/íŒ€í”Œ/Data/í•™ì‚¬ì¼ì •_ì •ë¦¬(2325).csv')\n",
        "academic['ì˜ì—…ì¼ì'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ì˜ì—…ì¼ì', how='left')\n",
        "test  = test.merge(academic, on='ì˜ì—…ì¼ì', how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test  = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test  = add_lag_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# Feature 15ê°œ\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend', 'acad_semester', 'acad_weekday',\n",
        "    'open_hours', 'acad_ceremony', 'acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "# =========================================================\n",
        "# log-space íƒ€ê¹ƒ\n",
        "# =========================================================\n",
        "train['ì¼ë§¤ì¶œ_log'] = np.log1p(train['ì¼ë§¤ì¶œ'])\n",
        "test['ì¼ë§¤ì¶œ_log']  = np.log1p(test['ì¼ë§¤ì¶œ'])\n",
        "\n",
        "# =========================================================\n",
        "# Feature Scaling\n",
        "# =========================================================\n",
        "scaler_X = MinMaxScaler()\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "\n",
        "# =========================================================\n",
        "# GRU í•™ìŠµ\n",
        "# =========================================================\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/50] Loss: {total_loss:.4f}\")\n",
        "\n",
        "# =========================================================\n",
        "# Test ì˜ˆì¸¡ (log â†’ real)\n",
        "# =========================================================\n",
        "test_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ì¼ë§¤ì¶œ'], test_pred)\n",
        "\n",
        "print(\"\\n====== ìµœì¢… ì„±ëŠ¥ (Stable GRU / log-space) ======\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "test['ì˜ˆì¸¡ë§¤ì¶œ'] = test_pred\n",
        "print(test[['ì˜ì—…ì¼ì', 'ì˜ˆì¸¡ë§¤ì¶œ']])\n",
        "\n",
        "test[['ì˜ì—…ì¼ì', 'ì˜ˆì¸¡ë§¤ì¶œ']].to_csv(\"prediction_gru_stable.csv\", index=False)\n",
        "\n",
        "print(\"\\nğŸ“Œ ì €ì¥ ì™„ë£Œ â†’ prediction_gru_stable.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPNP9oD8_r39",
        "outputId": "26b3495a-1751-4eb2-d2a0-bceff2f137bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] Loss: 1765.0869\n",
            "[Epoch 2/50] Loss: 747.7218\n",
            "[Epoch 3/50] Loss: 660.6371\n",
            "[Epoch 4/50] Loss: 645.6322\n",
            "[Epoch 5/50] Loss: 640.5663\n",
            "[Epoch 6/50] Loss: 636.4551\n",
            "[Epoch 7/50] Loss: 652.1385\n",
            "[Epoch 8/50] Loss: 640.7224\n",
            "[Epoch 9/50] Loss: 649.1341\n",
            "[Epoch 10/50] Loss: 644.4420\n",
            "[Epoch 11/50] Loss: 635.8274\n",
            "[Epoch 12/50] Loss: 644.2869\n",
            "[Epoch 13/50] Loss: 653.1559\n",
            "[Epoch 14/50] Loss: 645.7050\n",
            "[Epoch 15/50] Loss: 648.0791\n",
            "[Epoch 16/50] Loss: 650.4869\n",
            "[Epoch 17/50] Loss: 648.5586\n",
            "[Epoch 18/50] Loss: 648.6203\n",
            "[Epoch 19/50] Loss: 649.4565\n",
            "[Epoch 20/50] Loss: 647.7044\n",
            "[Epoch 21/50] Loss: 647.6448\n",
            "[Epoch 22/50] Loss: 637.8861\n",
            "[Epoch 23/50] Loss: 649.3445\n",
            "[Epoch 24/50] Loss: 653.6799\n",
            "[Epoch 25/50] Loss: 633.6795\n",
            "[Epoch 26/50] Loss: 652.5087\n",
            "[Epoch 27/50] Loss: 632.0541\n",
            "[Epoch 28/50] Loss: 641.6149\n",
            "[Epoch 29/50] Loss: 649.3116\n",
            "[Epoch 30/50] Loss: 637.9044\n",
            "[Epoch 31/50] Loss: 641.0362\n",
            "[Epoch 32/50] Loss: 640.7006\n",
            "[Epoch 33/50] Loss: 648.7731\n",
            "[Epoch 34/50] Loss: 632.2747\n",
            "[Epoch 35/50] Loss: 639.8986\n",
            "[Epoch 36/50] Loss: 643.5307\n",
            "[Epoch 37/50] Loss: 642.5718\n",
            "[Epoch 38/50] Loss: 640.2104\n",
            "[Epoch 39/50] Loss: 631.3588\n",
            "[Epoch 40/50] Loss: 623.6178\n",
            "[Epoch 41/50] Loss: 614.7675\n",
            "[Epoch 42/50] Loss: 577.8707\n",
            "[Epoch 43/50] Loss: 570.1127\n",
            "[Epoch 44/50] Loss: 516.5809\n",
            "[Epoch 45/50] Loss: 429.4005\n",
            "[Epoch 46/50] Loss: 362.6551\n",
            "[Epoch 47/50] Loss: 293.6314\n",
            "[Epoch 48/50] Loss: 277.7288\n",
            "[Epoch 49/50] Loss: 286.9442\n",
            "[Epoch 50/50] Loss: 279.7789\n",
            "\n",
            "====== ìµœì¢… ì„±ëŠ¥ (Stable GRU / log-space) ======\n",
            "MAE : 162657.79323050473\n",
            "RMSE: 216860.7420593933\n",
            "SMAPE: 199.99967429769566\n",
            "         ì˜ì—…ì¼ì      ì˜ˆì¸¡ë§¤ì¶œ\n",
            "0  2025-08-10  0.000000\n",
            "1  2025-08-11  0.746575\n",
            "2  2025-08-12  0.000000\n",
            "3  2025-08-13  0.405095\n",
            "4  2025-08-14  0.327476\n",
            "..        ...       ...\n",
            "78 2025-10-27  0.000000\n",
            "79 2025-10-28  2.094790\n",
            "80 2025-10-29  0.000000\n",
            "81 2025-10-30  0.395229\n",
            "82 2025-10-31  0.000000\n",
            "\n",
            "[83 rows x 2 columns]\n",
            "\n",
            "ğŸ“Œ ì €ì¥ ì™„ë£Œ â†’ prediction_gru_stable.csv\n"
          ]
        }
      ]
    }
  ]
}