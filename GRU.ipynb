{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**optimizer=adam (best)**"
      ],
      "metadata": {
        "id": "uW_FlbVzzVx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYArBrLKwttV",
        "outputId": "b01cd459-1697-42ec-e4f0-8416f097b531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] Loss: 0.8714\n",
            "[Epoch 2/50] Loss: 0.5253\n",
            "[Epoch 3/50] Loss: 0.4432\n",
            "[Epoch 4/50] Loss: 0.3666\n",
            "[Epoch 5/50] Loss: 0.3487\n",
            "[Epoch 6/50] Loss: 0.3307\n",
            "[Epoch 7/50] Loss: 0.3320\n",
            "[Epoch 8/50] Loss: 0.3193\n",
            "[Epoch 9/50] Loss: 0.3023\n",
            "[Epoch 10/50] Loss: 0.2984\n",
            "[Epoch 11/50] Loss: 0.2922\n",
            "[Epoch 12/50] Loss: 0.2591\n",
            "[Epoch 13/50] Loss: 0.2478\n",
            "[Epoch 14/50] Loss: 0.2435\n",
            "[Epoch 15/50] Loss: 0.2300\n",
            "[Epoch 16/50] Loss: 0.2335\n",
            "[Epoch 17/50] Loss: 0.2289\n",
            "[Epoch 18/50] Loss: 0.2137\n",
            "[Epoch 19/50] Loss: 0.2119\n",
            "[Epoch 20/50] Loss: 0.2062\n",
            "[Epoch 21/50] Loss: 0.2119\n",
            "[Epoch 22/50] Loss: 0.2106\n",
            "[Epoch 23/50] Loss: 0.2130\n",
            "[Epoch 24/50] Loss: 0.2027\n",
            "[Epoch 25/50] Loss: 0.2083\n",
            "[Epoch 26/50] Loss: 0.2082\n",
            "[Epoch 27/50] Loss: 0.2012\n",
            "[Epoch 28/50] Loss: 0.1947\n",
            "[Epoch 29/50] Loss: 0.2009\n",
            "[Epoch 30/50] Loss: 0.1999\n",
            "[Epoch 31/50] Loss: 0.2094\n",
            "[Epoch 32/50] Loss: 0.2004\n",
            "[Epoch 33/50] Loss: 0.1923\n",
            "[Epoch 34/50] Loss: 0.1984\n",
            "[Epoch 35/50] Loss: 0.1918\n",
            "[Epoch 36/50] Loss: 0.2006\n",
            "[Epoch 37/50] Loss: 0.1884\n",
            "[Epoch 38/50] Loss: 0.1917\n",
            "[Epoch 39/50] Loss: 0.1866\n",
            "[Epoch 40/50] Loss: 0.1959\n",
            "[Epoch 41/50] Loss: 0.1977\n",
            "[Epoch 42/50] Loss: 0.1912\n",
            "[Epoch 43/50] Loss: 0.1850\n",
            "[Epoch 44/50] Loss: 0.1799\n",
            "[Epoch 45/50] Loss: 0.1870\n",
            "[Epoch 46/50] Loss: 0.1854\n",
            "[Epoch 47/50] Loss: 0.1791\n",
            "[Epoch 48/50] Loss: 0.1936\n",
            "[Epoch 49/50] Loss: 0.2071\n",
            "[Epoch 50/50] Loss: 0.2062\n",
            "\n",
            "====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\n",
            "MAE : 57663.89634130994\n",
            "RMSE: 93487.57762916297\n",
            "SMAPE: 74.88710481461712\n",
            "         ÏòÅÏóÖÏùºÏûê           ÏòàÏ∏°Îß§Ï∂ú\n",
            "0  2025-08-10   55496.688945\n",
            "1  2025-08-11   87378.539070\n",
            "2  2025-08-12   65091.380958\n",
            "3  2025-08-13   46447.120981\n",
            "4  2025-08-14   29272.331217\n",
            "..        ...            ...\n",
            "78 2025-10-27  306648.547465\n",
            "79 2025-10-28  308867.956710\n",
            "80 2025-10-29  274780.548763\n",
            "81 2025-10-30  257709.832263\n",
            "82 2025-10-31  204987.322336\n",
            "\n",
            "[83 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed Í≥†Ï†ï\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ÏùºÎß§Ï∂ú'] = (\n",
        "        df['ÏùºÎß§Ï∂ú']\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ÏùºÎß§Ï∂ú'] = pd.to_numeric(df['ÏùºÎß§Ï∂ú'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['day'] = df['ÏòÅÏóÖÏùºÏûê'].dt.day\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ÏùºÎß§Ï∂ú'].shift(1)\n",
        "    df['lag7'] = df['ÏùºÎß§Ï∂ú'].shift(7)\n",
        "    df['lag14'] = df['ÏùºÎß§Ï∂ú'].shift(14)\n",
        "    df['lag28'] = df['ÏùºÎß§Ï∂ú'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ÏùºÎß§Ï∂ú'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# Sliding Window\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ÏùºÎß§Ï∂ú'].values\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================================================\n",
        "# ‚≠ê‚≠ê 2-Layer GRU Î™®Îç∏ ‚≠ê‚≠ê\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# =========================================================\n",
        "# Autoregressive ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, df_train, df_test, seq_len, feature_cols, scaler_X, scaler_y):\n",
        "\n",
        "    history = df_train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        row = df_test.iloc[i].copy()\n",
        "        last_vals = history['ÏùºÎß§Ï∂ú'].values\n",
        "\n",
        "        # lag ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['lag1'] = last_vals[-1]\n",
        "        row['lag7'] = last_vals[-7]\n",
        "        row['lag14'] = last_vals[-14]\n",
        "        row['lag28'] = last_vals[-28]\n",
        "\n",
        "        # rolling ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['roll_mean7']  = pd.Series(last_vals[-7:]).mean()\n",
        "        row['roll_mean14'] = pd.Series(last_vals[-14:]).mean()\n",
        "        row['roll_mean28'] = pd.Series(last_vals[-28:]).mean()\n",
        "\n",
        "        row['roll_std7']  = pd.Series(last_vals[-7:]).std()\n",
        "        row['roll_std28'] = pd.Series(last_vals[-28:]).std()\n",
        "\n",
        "        # ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\n",
        "        seq_df = history.tail(seq_len).copy()\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # ÏòàÏ∏°\n",
        "        pred_scaled = model(X).item()\n",
        "        pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "        row['ÏùºÎß§Ï∂ú'] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# =========================================================\n",
        "# üî• Î©îÏù∏ Ïã§Ìñâ\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_train_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test = clean_sales(test)\n",
        "\n",
        "# ÌïôÏÇ¨ÏùºÏ†ï merge\n",
        "academic = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/ÌïôÏÇ¨ÏùºÏ†ï_Ï†ïÎ¶¨(2325).csv')\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test = add_lag_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# ÏµúÏ¢Ö Feature 15Í∞ú\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend', 'acad_semester', 'acad_weekday',\n",
        "    'open_hours', 'acad_ceremony', 'acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "# Scaling\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "train[['ÏùºÎß§Ï∂ú']] = scaler_y.fit_transform(train[['ÏùºÎß§Ï∂ú']])\n",
        "\n",
        "# =========================================================\n",
        "# Sequence Length = 28\n",
        "# =========================================================\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ‚≠ê 2-Layer GRU Î™®Îç∏\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ÌïôÏäµ 50 epochs\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/50] Loss: {total_loss:.4f}\")\n",
        "\n",
        "# =========================================================\n",
        "# Test ÏòàÏ∏°\n",
        "# =========================================================\n",
        "test_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X, scaler_y)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], test_pred)\n",
        "\n",
        "print(\"\\n====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "test['ÏòàÏ∏°Îß§Ï∂ú'] = test_pred\n",
        "print(test[['ÏòÅÏóÖÏùºÏûê', 'ÏòàÏ∏°Îß§Ï∂ú']])\n",
        "\n",
        "test[['ÏòÅÏóÖÏùºÏûê', 'ÏòàÏ∏°Îß§Ï∂ú']].to_csv(\"prediction_full_output.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**optimizer=RMSprop**"
      ],
      "metadata": {
        "id": "PMZL9wAVzAcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed Í≥†Ï†ï\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ÏùºÎß§Ï∂ú'] = (\n",
        "        df['ÏùºÎß§Ï∂ú']\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ÏùºÎß§Ï∂ú'] = pd.to_numeric(df['ÏùºÎß§Ï∂ú'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['day'] = df['ÏòÅÏóÖÏùºÏûê'].dt.day\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ÏùºÎß§Ï∂ú'].shift(1)\n",
        "    df['lag7'] = df['ÏùºÎß§Ï∂ú'].shift(7)\n",
        "    df['lag14'] = df['ÏùºÎß§Ï∂ú'].shift(14)\n",
        "    df['lag28'] = df['ÏùºÎß§Ï∂ú'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ÏùºÎß§Ï∂ú'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# Sliding Window\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ÏùºÎß§Ï∂ú'].values\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================================================\n",
        "# ‚≠ê‚≠ê 2-Layer GRU Î™®Îç∏ ‚≠ê‚≠ê\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# =========================================================\n",
        "# Autoregressive ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, df_train, df_test, seq_len, feature_cols, scaler_X, scaler_y):\n",
        "\n",
        "    history = df_train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        row = df_test.iloc[i].copy()\n",
        "        last_vals = history['ÏùºÎß§Ï∂ú'].values\n",
        "\n",
        "        # lag ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['lag1'] = last_vals[-1]\n",
        "        row['lag7'] = last_vals[-7]\n",
        "        row['lag14'] = last_vals[-14]\n",
        "        row['lag28'] = last_vals[-28]\n",
        "\n",
        "        # rolling ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['roll_mean7']  = pd.Series(last_vals[-7:]).mean()\n",
        "        row['roll_mean14'] = pd.Series(last_vals[-14:]).mean()\n",
        "        row['roll_mean28'] = pd.Series(last_vals[-28:]).mean()\n",
        "\n",
        "        row['roll_std7']  = pd.Series(last_vals[-7:]).std()\n",
        "        row['roll_std28'] = pd.Series(last_vals[-28:]).std()\n",
        "\n",
        "        # ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\n",
        "        seq_df = history.tail(seq_len).copy()\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # ÏòàÏ∏°\n",
        "        pred_scaled = model(X).item()\n",
        "        pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "        row['ÏùºÎß§Ï∂ú'] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# =========================================================\n",
        "# üî• Î©îÏù∏ Ïã§Ìñâ\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_train_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test = clean_sales(test)\n",
        "\n",
        "# ÌïôÏÇ¨ÏùºÏ†ï merge\n",
        "academic = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/ÌïôÏÇ¨ÏùºÏ†ï_Ï†ïÎ¶¨(2325).csv')\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test = add_lag_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# ÏµúÏ¢Ö Feature 15Í∞ú\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend', 'acad_semester', 'acad_weekday',\n",
        "    'open_hours', 'acad_ceremony', 'acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "# Scaling\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "train[['ÏùºÎß§Ï∂ú']] = scaler_y.fit_transform(train[['ÏùºÎß§Ï∂ú']])\n",
        "\n",
        "# =========================================================\n",
        "# Sequence Length = 28\n",
        "# =========================================================\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ‚≠ê 2-Layer GRU Î™®Îç∏\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "# ÌïôÏäµ 50 epochs\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/50] Loss: {total_loss:.4f}\")\n",
        "\n",
        "# =========================================================\n",
        "# Test ÏòàÏ∏°\n",
        "# =========================================================\n",
        "test_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X, scaler_y)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], test_pred)\n",
        "\n",
        "print(\"\\n====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "test['ÏòàÏ∏°Îß§Ï∂ú'] = test_pred\n",
        "print(test[['ÏòÅÏóÖÏùºÏûê', 'ÏòàÏ∏°Îß§Ï∂ú']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIsEIIQdzFGp",
        "outputId": "394171e8-25fa-4409-f024-fc7472105ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] Loss: 1.0675\n",
            "[Epoch 2/50] Loss: 0.4683\n",
            "[Epoch 3/50] Loss: 0.4144\n",
            "[Epoch 4/50] Loss: 0.3798\n",
            "[Epoch 5/50] Loss: 0.3650\n",
            "[Epoch 6/50] Loss: 0.3299\n",
            "[Epoch 7/50] Loss: 0.3347\n",
            "[Epoch 8/50] Loss: 0.3130\n",
            "[Epoch 9/50] Loss: 0.2906\n",
            "[Epoch 10/50] Loss: 0.2704\n",
            "[Epoch 11/50] Loss: 0.2747\n",
            "[Epoch 12/50] Loss: 0.2480\n",
            "[Epoch 13/50] Loss: 0.2488\n",
            "[Epoch 14/50] Loss: 0.2473\n",
            "[Epoch 15/50] Loss: 0.2613\n",
            "[Epoch 16/50] Loss: 0.2499\n",
            "[Epoch 17/50] Loss: 0.2300\n",
            "[Epoch 18/50] Loss: 0.2316\n",
            "[Epoch 19/50] Loss: 0.2409\n",
            "[Epoch 20/50] Loss: 0.2166\n",
            "[Epoch 21/50] Loss: 0.2198\n",
            "[Epoch 22/50] Loss: 0.2175\n",
            "[Epoch 23/50] Loss: 0.2230\n",
            "[Epoch 24/50] Loss: 0.2201\n",
            "[Epoch 25/50] Loss: 0.2067\n",
            "[Epoch 26/50] Loss: 0.2264\n",
            "[Epoch 27/50] Loss: 0.2102\n",
            "[Epoch 28/50] Loss: 0.2225\n",
            "[Epoch 29/50] Loss: 0.2137\n",
            "[Epoch 30/50] Loss: 0.2131\n",
            "[Epoch 31/50] Loss: 0.2067\n",
            "[Epoch 32/50] Loss: 0.2129\n",
            "[Epoch 33/50] Loss: 0.2074\n",
            "[Epoch 34/50] Loss: 0.2247\n",
            "[Epoch 35/50] Loss: 0.2058\n",
            "[Epoch 36/50] Loss: 0.2003\n",
            "[Epoch 37/50] Loss: 0.1976\n",
            "[Epoch 38/50] Loss: 0.1955\n",
            "[Epoch 39/50] Loss: 0.1987\n",
            "[Epoch 40/50] Loss: 0.2199\n",
            "[Epoch 41/50] Loss: 0.2120\n",
            "[Epoch 42/50] Loss: 0.1968\n",
            "[Epoch 43/50] Loss: 0.2096\n",
            "[Epoch 44/50] Loss: 0.1960\n",
            "[Epoch 45/50] Loss: 0.1990\n",
            "[Epoch 46/50] Loss: 0.1903\n",
            "[Epoch 47/50] Loss: 0.1963\n",
            "[Epoch 48/50] Loss: 0.1917\n",
            "[Epoch 49/50] Loss: 0.2101\n",
            "[Epoch 50/50] Loss: 0.2017\n",
            "\n",
            "====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\n",
            "MAE : 63369.262801842335\n",
            "RMSE: 99818.20841506236\n",
            "SMAPE: 70.99659603577723\n",
            "         ÏòÅÏóÖÏùºÏûê           ÏòàÏ∏°Îß§Ï∂ú\n",
            "0  2025-08-10   39384.962174\n",
            "1  2025-08-11   50055.255494\n",
            "2  2025-08-12   23276.773293\n",
            "3  2025-08-13    3834.041777\n",
            "4  2025-08-14       0.000000\n",
            "..        ...            ...\n",
            "78 2025-10-27  168754.564489\n",
            "79 2025-10-28  191202.199522\n",
            "80 2025-10-29  178002.588236\n",
            "81 2025-10-30  163758.132407\n",
            "82 2025-10-31  128440.243828\n",
            "\n",
            "[83 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SGD**"
      ],
      "metadata": {
        "id": "Vy9rLkLzzdR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed Í≥†Ï†ï\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ÏùºÎß§Ï∂ú'] = (\n",
        "        df['ÏùºÎß§Ï∂ú']\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ÏùºÎß§Ï∂ú'] = pd.to_numeric(df['ÏùºÎß§Ï∂ú'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['day'] = df['ÏòÅÏóÖÏùºÏûê'].dt.day\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ÏùºÎß§Ï∂ú'].shift(1)\n",
        "    df['lag7'] = df['ÏùºÎß§Ï∂ú'].shift(7)\n",
        "    df['lag14'] = df['ÏùºÎß§Ï∂ú'].shift(14)\n",
        "    df['lag28'] = df['ÏùºÎß§Ï∂ú'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ÏùºÎß§Ï∂ú'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# Sliding Window\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ÏùºÎß§Ï∂ú'].values\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================================================\n",
        "# ‚≠ê‚≠ê 2-Layer GRU Î™®Îç∏ ‚≠ê‚≠ê\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# =========================================================\n",
        "# Autoregressive ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, df_train, df_test, seq_len, feature_cols, scaler_X, scaler_y):\n",
        "\n",
        "    history = df_train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        row = df_test.iloc[i].copy()\n",
        "        last_vals = history['ÏùºÎß§Ï∂ú'].values\n",
        "\n",
        "        # lag ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['lag1'] = last_vals[-1]\n",
        "        row['lag7'] = last_vals[-7]\n",
        "        row['lag14'] = last_vals[-14]\n",
        "        row['lag28'] = last_vals[-28]\n",
        "\n",
        "        # rolling ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['roll_mean7']  = pd.Series(last_vals[-7:]).mean()\n",
        "        row['roll_mean14'] = pd.Series(last_vals[-14:]).mean()\n",
        "        row['roll_mean28'] = pd.Series(last_vals[-28:]).mean()\n",
        "\n",
        "        row['roll_std7']  = pd.Series(last_vals[-7:]).std()\n",
        "        row['roll_std28'] = pd.Series(last_vals[-28:]).std()\n",
        "\n",
        "        # ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\n",
        "        seq_df = history.tail(seq_len).copy()\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # ÏòàÏ∏°\n",
        "        pred_scaled = model(X).item()\n",
        "        pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "        row['ÏùºÎß§Ï∂ú'] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# =========================================================\n",
        "# üî• Î©îÏù∏ Ïã§Ìñâ\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_train_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test = clean_sales(test)\n",
        "\n",
        "# ÌïôÏÇ¨ÏùºÏ†ï merge\n",
        "academic = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/ÌïôÏÇ¨ÏùºÏ†ï_Ï†ïÎ¶¨(2325).csv')\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test = add_lag_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# ÏµúÏ¢Ö Feature 15Í∞ú\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend', 'acad_semester', 'acad_weekday',\n",
        "    'open_hours', 'acad_ceremony', 'acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "# Scaling\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "train[['ÏùºÎß§Ï∂ú']] = scaler_y.fit_transform(train[['ÏùºÎß§Ï∂ú']])\n",
        "\n",
        "# =========================================================\n",
        "# Sequence Length = 28\n",
        "# =========================================================\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ‚≠ê 2-Layer GRU Î™®Îç∏\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# ‚≠ê Adam ‚Üí SGD(m=0.9)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# ÌïôÏäµ 50 epochs\n",
        "for epoch in range(70):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/70] Loss: {total_loss:.4f}\")\n",
        "\n",
        "# =========================================================\n",
        "# Test ÏòàÏ∏°\n",
        "# =========================================================\n",
        "test_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X, scaler_y)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], test_pred)\n",
        "\n",
        "print(\"\\n====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "test['ÏòàÏ∏°Îß§Ï∂ú'] = test_pred\n",
        "print(test[['ÏòÅÏóÖÏùºÏûê', 'ÏòàÏ∏°Îß§Ï∂ú']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eo9l5DHzgCt",
        "outputId": "7f6220e7-d5cc-4096-da34-fedeae6b28e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/70] Loss: 1.4417\n",
            "[Epoch 2/70] Loss: 0.7957\n",
            "[Epoch 3/70] Loss: 0.7591\n",
            "[Epoch 4/70] Loss: 0.7328\n",
            "[Epoch 5/70] Loss: 0.7144\n",
            "[Epoch 6/70] Loss: 0.7033\n",
            "[Epoch 7/70] Loss: 0.6885\n",
            "[Epoch 8/70] Loss: 0.6853\n",
            "[Epoch 9/70] Loss: 0.6709\n",
            "[Epoch 10/70] Loss: 0.6628\n",
            "[Epoch 11/70] Loss: 0.6571\n",
            "[Epoch 12/70] Loss: 0.6457\n",
            "[Epoch 13/70] Loss: 0.6435\n",
            "[Epoch 14/70] Loss: 0.6401\n",
            "[Epoch 15/70] Loss: 0.6268\n",
            "[Epoch 16/70] Loss: 0.6258\n",
            "[Epoch 17/70] Loss: 0.6158\n",
            "[Epoch 18/70] Loss: 0.6135\n",
            "[Epoch 19/70] Loss: 0.6080\n",
            "[Epoch 20/70] Loss: 0.5912\n",
            "[Epoch 21/70] Loss: 0.5841\n",
            "[Epoch 22/70] Loss: 0.5867\n",
            "[Epoch 23/70] Loss: 0.5823\n",
            "[Epoch 24/70] Loss: 0.5796\n",
            "[Epoch 25/70] Loss: 0.5725\n",
            "[Epoch 26/70] Loss: 0.5676\n",
            "[Epoch 27/70] Loss: 0.5763\n",
            "[Epoch 28/70] Loss: 0.5706\n",
            "[Epoch 29/70] Loss: 0.5628\n",
            "[Epoch 30/70] Loss: 0.5652\n",
            "[Epoch 31/70] Loss: 0.5746\n",
            "[Epoch 32/70] Loss: 0.5586\n",
            "[Epoch 33/70] Loss: 0.5651\n",
            "[Epoch 34/70] Loss: 0.5490\n",
            "[Epoch 35/70] Loss: 0.5527\n",
            "[Epoch 36/70] Loss: 0.5599\n",
            "[Epoch 37/70] Loss: 0.5525\n",
            "[Epoch 38/70] Loss: 0.5501\n",
            "[Epoch 39/70] Loss: 0.5488\n",
            "[Epoch 40/70] Loss: 0.5615\n",
            "[Epoch 41/70] Loss: 0.5543\n",
            "[Epoch 42/70] Loss: 0.5443\n",
            "[Epoch 43/70] Loss: 0.5378\n",
            "[Epoch 44/70] Loss: 0.5348\n",
            "[Epoch 45/70] Loss: 0.5292\n",
            "[Epoch 46/70] Loss: 0.5278\n",
            "[Epoch 47/70] Loss: 0.5352\n",
            "[Epoch 48/70] Loss: 0.5267\n",
            "[Epoch 49/70] Loss: 0.5550\n",
            "[Epoch 50/70] Loss: 0.5209\n",
            "[Epoch 51/70] Loss: 0.5237\n",
            "[Epoch 52/70] Loss: 0.5248\n",
            "[Epoch 53/70] Loss: 0.5251\n",
            "[Epoch 54/70] Loss: 0.5219\n",
            "[Epoch 55/70] Loss: 0.5293\n",
            "[Epoch 56/70] Loss: 0.5192\n",
            "[Epoch 57/70] Loss: 0.5302\n",
            "[Epoch 58/70] Loss: 0.5215\n",
            "[Epoch 59/70] Loss: 0.5184\n",
            "[Epoch 60/70] Loss: 0.5235\n",
            "[Epoch 61/70] Loss: 0.5300\n",
            "[Epoch 62/70] Loss: 0.5353\n",
            "[Epoch 63/70] Loss: 0.5067\n",
            "[Epoch 64/70] Loss: 0.5313\n",
            "[Epoch 65/70] Loss: 0.5221\n",
            "[Epoch 66/70] Loss: 0.5120\n",
            "[Epoch 67/70] Loss: 0.5086\n",
            "[Epoch 68/70] Loss: 0.5063\n",
            "[Epoch 69/70] Loss: 0.5031\n",
            "[Epoch 70/70] Loss: 0.5044\n",
            "\n",
            "====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\n",
            "MAE : 104057.99903670797\n",
            "RMSE: 124139.71892327619\n",
            "SMAPE: 101.7037832372385\n",
            "         ÏòÅÏóÖÏùºÏûê           ÏòàÏ∏°Îß§Ï∂ú\n",
            "0  2025-08-10   64338.114133\n",
            "1  2025-08-11   75313.521449\n",
            "2  2025-08-12   72347.020742\n",
            "3  2025-08-13   59733.003525\n",
            "4  2025-08-14   44492.862199\n",
            "..        ...            ...\n",
            "78 2025-10-27  153559.042606\n",
            "79 2025-10-28  163237.240845\n",
            "80 2025-10-29  154883.796957\n",
            "81 2025-10-30  138061.205524\n",
            "82 2025-10-31  120127.806457\n",
            "\n",
            "[83 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adagrad**"
      ],
      "metadata": {
        "id": "kxR08JyRzgRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed Í≥†Ï†ï\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ÏùºÎß§Ï∂ú'] = (\n",
        "        df['ÏùºÎß§Ï∂ú']\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ÏùºÎß§Ï∂ú'] = pd.to_numeric(df['ÏùºÎß§Ï∂ú'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['day'] = df['ÏòÅÏóÖÏùºÏûê'].dt.day\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ÏùºÎß§Ï∂ú'].shift(1)\n",
        "    df['lag7'] = df['ÏùºÎß§Ï∂ú'].shift(7)\n",
        "    df['lag14'] = df['ÏùºÎß§Ï∂ú'].shift(14)\n",
        "    df['lag28'] = df['ÏùºÎß§Ï∂ú'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ÏùºÎß§Ï∂ú'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "# =========================================================\n",
        "# Sliding Window\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ÏùºÎß§Ï∂ú'].values\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# =========================================================\n",
        "# ‚≠ê‚≠ê 2-Layer GRU Î™®Îç∏ ‚≠ê‚≠ê\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "# =========================================================\n",
        "# Autoregressive ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, df_train, df_test, seq_len, feature_cols, scaler_X, scaler_y):\n",
        "\n",
        "    history = df_train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        row = df_test.iloc[i].copy()\n",
        "        last_vals = history['ÏùºÎß§Ï∂ú'].values\n",
        "\n",
        "        # lag ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['lag1'] = last_vals[-1]\n",
        "        row['lag7'] = last_vals[-7]\n",
        "        row['lag14'] = last_vals[-14]\n",
        "        row['lag28'] = last_vals[-28]\n",
        "\n",
        "        # rolling ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        row['roll_mean7']  = pd.Series(last_vals[-7:]).mean()\n",
        "        row['roll_mean14'] = pd.Series(last_vals[-14:]).mean()\n",
        "        row['roll_mean28'] = pd.Series(last_vals[-28:]).mean()\n",
        "\n",
        "        row['roll_std7']  = pd.Series(last_vals[-7:]).std()\n",
        "        row['roll_std28'] = pd.Series(last_vals[-28:]).std()\n",
        "\n",
        "        # ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\n",
        "        seq_df = history.tail(seq_len).copy()\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # ÏòàÏ∏°\n",
        "        pred_scaled = model(X).item()\n",
        "        pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "        row['ÏùºÎß§Ï∂ú'] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# =========================================================\n",
        "# üî• Î©îÏù∏ Ïã§Ìñâ\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_train_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test = clean_sales(test)\n",
        "\n",
        "# ÌïôÏÇ¨ÏùºÏ†ï merge\n",
        "academic = pd.read_csv('/content/drive/MyDrive/Í∏∞Í≥ÑÌïôÏäµ/ÌåÄÌîå/Data/ÌïôÏÇ¨ÏùºÏ†ï_Ï†ïÎ¶¨(2325).csv')\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test = add_lag_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# ÏµúÏ¢Ö Feature 15Í∞ú\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend', 'acad_semester', 'acad_weekday',\n",
        "    'open_hours', 'acad_ceremony', 'acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "# Scaling\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "train[['ÏùºÎß§Ï∂ú']] = scaler_y.fit_transform(train[['ÏùºÎß§Ï∂ú']])\n",
        "\n",
        "# =========================================================\n",
        "# Sequence Length = 28\n",
        "# =========================================================\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 2-Layer GRU Î™®Îç∏\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Adagrad\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
        "\n",
        "# ÌïôÏäµ 50 epochs\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/50] Loss: {total_loss:.4f}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Test ÏòàÏ∏°\n",
        "# =========================================================\n",
        "test_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X, scaler_y)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], test_pred)\n",
        "\n",
        "print(\"\\n====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\")\n",
        "print(\"MAE :\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "test['ÏòàÏ∏°Îß§Ï∂ú'] = test_pred\n",
        "print(test[['ÏòÅÏóÖÏùºÏûê', 'ÏòàÏ∏°Îß§Ï∂ú']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLtd_2mlziJX",
        "outputId": "05f3228b-5fa3-48fe-90dd-7da05ddd9e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] Loss: 0.8402\n",
            "[Epoch 2/50] Loss: 0.6291\n",
            "[Epoch 3/50] Loss: 0.5809\n",
            "[Epoch 4/50] Loss: 0.5459\n",
            "[Epoch 5/50] Loss: 0.5305\n",
            "[Epoch 6/50] Loss: 0.5117\n",
            "[Epoch 7/50] Loss: 0.4972\n",
            "[Epoch 8/50] Loss: 0.4938\n",
            "[Epoch 9/50] Loss: 0.4787\n",
            "[Epoch 10/50] Loss: 0.4707\n",
            "[Epoch 11/50] Loss: 0.4673\n",
            "[Epoch 12/50] Loss: 0.4472\n",
            "[Epoch 13/50] Loss: 0.4424\n",
            "[Epoch 14/50] Loss: 0.4389\n",
            "[Epoch 15/50] Loss: 0.4242\n",
            "[Epoch 16/50] Loss: 0.4227\n",
            "[Epoch 17/50] Loss: 0.4116\n",
            "[Epoch 18/50] Loss: 0.4053\n",
            "[Epoch 19/50] Loss: 0.3967\n",
            "[Epoch 20/50] Loss: 0.3838\n",
            "[Epoch 21/50] Loss: 0.3793\n",
            "[Epoch 22/50] Loss: 0.3837\n",
            "[Epoch 23/50] Loss: 0.3730\n",
            "[Epoch 24/50] Loss: 0.3713\n",
            "[Epoch 25/50] Loss: 0.3665\n",
            "[Epoch 26/50] Loss: 0.3650\n",
            "[Epoch 27/50] Loss: 0.3649\n",
            "[Epoch 28/50] Loss: 0.3641\n",
            "[Epoch 29/50] Loss: 0.3584\n",
            "[Epoch 30/50] Loss: 0.3604\n",
            "[Epoch 31/50] Loss: 0.3627\n",
            "[Epoch 32/50] Loss: 0.3516\n",
            "[Epoch 33/50] Loss: 0.3550\n",
            "[Epoch 34/50] Loss: 0.3492\n",
            "[Epoch 35/50] Loss: 0.3520\n",
            "[Epoch 36/50] Loss: 0.3543\n",
            "[Epoch 37/50] Loss: 0.3469\n",
            "[Epoch 38/50] Loss: 0.3446\n",
            "[Epoch 39/50] Loss: 0.3426\n",
            "[Epoch 40/50] Loss: 0.3522\n",
            "[Epoch 41/50] Loss: 0.3477\n",
            "[Epoch 42/50] Loss: 0.3428\n",
            "[Epoch 43/50] Loss: 0.3413\n",
            "[Epoch 44/50] Loss: 0.3384\n",
            "[Epoch 45/50] Loss: 0.3401\n",
            "[Epoch 46/50] Loss: 0.3390\n",
            "[Epoch 47/50] Loss: 0.3382\n",
            "[Epoch 48/50] Loss: 0.3402\n",
            "[Epoch 49/50] Loss: 0.3585\n",
            "[Epoch 50/50] Loss: 0.3318\n",
            "\n",
            "====== ÏµúÏ¢Ö ÏÑ±Îä• (2-Layer GRU + 15 features + seq_len=28) ======\n",
            "MAE : 81913.46959883728\n",
            "RMSE: 114216.06290200916\n",
            "SMAPE: 83.50886024737535\n",
            "         ÏòÅÏóÖÏùºÏûê           ÏòàÏ∏°Îß§Ï∂ú\n",
            "0  2025-08-10   52784.361110\n",
            "1  2025-08-11   70080.444624\n",
            "2  2025-08-12  112240.703834\n",
            "3  2025-08-13  104565.552002\n",
            "4  2025-08-14   62109.415580\n",
            "..        ...            ...\n",
            "78 2025-10-27  152272.090785\n",
            "79 2025-10-28  278310.185623\n",
            "80 2025-10-29  286089.913452\n",
            "81 2025-10-30  210523.509729\n",
            "82 2025-10-31  111615.844057\n",
            "\n",
            "[83 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}