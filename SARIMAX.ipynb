{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed Í≥†Ï†ï\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================================================\n",
        "# 1) ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "# =========================================================\n",
        "# 2) Autoregressive SARIMAX ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def autoregressive_sarimax(train, test, exog_cols):\n",
        "\n",
        "    history = train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(test)):\n",
        "\n",
        "        exog_row = test.loc[[i], exog_cols]\n",
        "        exog_row = exog_row.fillna(0)\n",
        "\n",
        "        model = SARIMAX(\n",
        "            history['ÏùºÎß§Ï∂ú'],\n",
        "            exog=history[exog_cols],\n",
        "            order=(1,1,1),\n",
        "            seasonal_order=(1,1,1,7)\n",
        "        )\n",
        "\n",
        "        fit = model.fit(disp=False)\n",
        "\n",
        "        forecast = fit.predict(\n",
        "            start=len(history),\n",
        "            end=len(history),\n",
        "            exog=exog_row\n",
        "        )\n",
        "\n",
        "        pred = float(forecast.values[0])\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "        new_row = test.iloc[i].copy()\n",
        "        new_row['ÏùºÎß§Ï∂ú'] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# =========================================================\n",
        "# 3) Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/POS_train_val.csv')\n",
        "test  = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test ['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test ['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train['ÏùºÎß§Ï∂ú'] = train['ÏùºÎß§Ï∂ú'].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "test ['ÏùºÎß§Ï∂ú'] = test ['ÏùºÎß§Ï∂ú'].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "\n",
        "# =========================================================\n",
        "# 4) ÌïôÏÇ¨ÏùºÏ†ï merge\n",
        "# =========================================================\n",
        "academic = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/·Ñí·Ö°·Ü®·Ñâ·Ö°·Ñã·Öµ·ÜØ·Ñå·Ö•·Üº_·Ñå·Ö•·Üº·ÑÖ·Öµ(2325).csv')\n",
        "\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test  = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "# =========================================================\n",
        "# 5) ÎÇ†Ïßú Í∏∞Î∞ò ÌîºÏ≤ò Ï∂îÍ∞Ä\n",
        "# =========================================================\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "train = add_date_features(train)\n",
        "test  = add_date_features(test)\n",
        "\n",
        "# =========================================================\n",
        "# 6) SARIMAX exog feature ÏÑ†ÌÉù (lag/rolling Ï†úÍ±∞)\n",
        "# =========================================================\n",
        "exog_cols = [\n",
        "    'acad_weekend','acad_semester','acad_weekday',\n",
        "    'acad_holiday','acad_seasonal','acad_exam',\n",
        "    'acad_ceremony','acad_dormitory',\n",
        "    'month','weekday','is_weekend','open_hours'\n",
        "]\n",
        "\n",
        "train[exog_cols] = train[exog_cols].fillna(0)\n",
        "test [exog_cols] = test [exog_cols].fillna(0)\n",
        "\n",
        "# =========================================================\n",
        "# 7) SARIMAX ÏòàÏ∏°\n",
        "# =========================================================\n",
        "test_pred = autoregressive_sarimax(train, test, exog_cols)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], test_pred)\n",
        "\n",
        "print(\"\\n====== SARIMAX Í≤∞Í≥º (lag/rolling Ï†úÍ±∞) ======\")\n",
        "print(\"MAE  :\", mae)\n",
        "print(\"RMSE :\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "# =========================================================\n",
        "# 8) CSV Ï†ÄÏû•\n",
        "# =========================================================\n",
        "output = test[['ÏòÅÏóÖÏùºÏûê']].copy()\n",
        "output['ÏòàÏ∏°Îß§Ï∂ú'] = test_pred\n",
        "output.to_csv(\"sarimax_prediction_output.csv\", index=False)\n",
        "\n",
        "print(\"\\nCSV ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å ‚Üí sarimax_prediction_output.csv\")\n",
        "print(output.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IyG-6W5UWvD",
        "outputId": "fb627734-d65c-48f9-eae1-533510d33048"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== SARIMAX Í≤∞Í≥º (lag/rolling Ï†úÍ±∞) ======\n",
            "MAE  : 77898.79778632565\n",
            "RMSE : 98289.41421725308\n",
            "SMAPE: 76.65960579041068\n",
            "\n",
            "CSV ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å ‚Üí sarimax_prediction_output.csv\n",
            "        ÏòÅÏóÖÏùºÏûê           ÏòàÏ∏°Îß§Ï∂ú\n",
            "0 2025-08-10       0.000000\n",
            "1 2025-08-11  185742.622154\n",
            "2 2025-08-12  122451.625520\n",
            "3 2025-08-13  162148.301086\n",
            "4 2025-08-14  153032.472745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏûêÎèô ÌÉêÏÉâ**"
      ],
      "metadata": {
        "id": "yblYjDYkZ8lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# ================================\n",
        "# Seed\n",
        "# ================================\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# ================================\n",
        "# ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# ================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    mae  = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smp  = smape(actual, pred)\n",
        "    return mae, rmse, smp\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 1) SARIMAX AIC Í∏∞Î∞ò ÏûêÎèô ÌÉêÏÉâÍ∏∞\n",
        "# =========================================================\n",
        "def sarimax_auto_search(train_df, exog_cols):\n",
        "\n",
        "    y = train_df[\"ÏùºÎß§Ï∂ú\"]\n",
        "    exog = train_df[exog_cols]\n",
        "\n",
        "    # Í≤ÄÏÉâ Í≥µÍ∞Ñ (ÎÑàÎ¨¥ ÎÑìÌûàÎ©¥ Í≥ºÎ∂ÄÌïò! Ïã§Î¨¥ÏóêÏÑúÎèÑ Ïù¥Î†áÍ≤å ÏîÄ)\n",
        "    p = d = q = range(0, 2)\n",
        "    P = D = Q = range(0, 2)\n",
        "    seasonal_period = [7]   # ÏöîÏùº Ìå®ÌÑ¥\n",
        "\n",
        "    best_aic = np.inf\n",
        "    best_param = None\n",
        "    best_model = None\n",
        "\n",
        "    search_space = list(itertools.product(p, d, q, P, D, Q, seasonal_period))\n",
        "\n",
        "    print(f\"Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: {len(search_space)}\\n\")\n",
        "\n",
        "    for (p_, d_, q_, P_, D_, Q_, s_) in search_space:\n",
        "\n",
        "        try:\n",
        "            model = SARIMAX(\n",
        "                y,\n",
        "                exog=exog,\n",
        "                order=(p_, d_, q_),\n",
        "                seasonal_order=(P_, D_, Q_, s_),\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False\n",
        "            )\n",
        "\n",
        "            fit = model.fit(disp=False)\n",
        "\n",
        "            if fit.aic < best_aic:\n",
        "                best_aic = fit.aic\n",
        "                best_param = (p_, d_, q_, P_, D_, Q_, s_)\n",
        "                best_model = fit\n",
        "\n",
        "                print(f\"üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: {best_aic:.2f} ‚Üí {best_param}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    print(\"\\n==========================\")\n",
        "    print(\"üìå ÏµúÏ¢Ö ÏÑ†ÌÉùÎêú SARIMAX Î™®Îç∏\")\n",
        "    print(\"order =\", best_param[:3])\n",
        "    print(\"seasonal_order =\", best_param[3:])\n",
        "    print(\"AIC =\", best_aic)\n",
        "    print(\"==========================\\n\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2) Auto-Regressive ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def sarimax_forecast_autoreg(train_df, test_df, exog_cols, best_model):\n",
        "\n",
        "    history = train_df.copy()\n",
        "    preds = []\n",
        "\n",
        "    # üî• ÏµúÏã† statsmodels Î≤ÑÏ†ÑÏóêÏÑú ÏßÄÏõêÌïòÎäî order Ï∂îÏ∂ú Î∞©Ïãù\n",
        "    order = best_model.model.order\n",
        "    seasonal_order = best_model.model.seasonal_order\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "\n",
        "        exog_next = test_df.loc[[i], exog_cols].fillna(0)\n",
        "\n",
        "        model = SARIMAX(\n",
        "            history[\"ÏùºÎß§Ï∂ú\"],\n",
        "            exog=history[exog_cols],\n",
        "            order=order,\n",
        "            seasonal_order=seasonal_order,\n",
        "            enforce_stationarity=False,\n",
        "            enforce_invertibility=False\n",
        "        )\n",
        "\n",
        "        fit = model.fit(disp=False)\n",
        "\n",
        "        forecast = fit.predict(\n",
        "            start=len(history),\n",
        "            end=len(history),\n",
        "            exog=exog_next\n",
        "        )\n",
        "\n",
        "        pred = float(forecast.values[0])\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "        new_row = test_df.iloc[i].copy()\n",
        "        new_row[\"ÏùºÎß§Ï∂ú\"] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3) Îç∞Ïù¥ÌÑ∞ Î°úÎìú (Í≤ΩÎ°ú Î≥ÄÍ≤Ω ÏóÜÏùå)\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/POS_train_val.csv')\n",
        "test  = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test ['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test ['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train['ÏùºÎß§Ï∂ú'] = train['ÏùºÎß§Ï∂ú'].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "test ['ÏùºÎß§Ï∂ú'] = test ['ÏùºÎß§Ï∂ú'].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4) ÌïôÏÇ¨ÏùºÏ†ï merge (Í∑∏ÎåÄÎ°ú)\n",
        "# =========================================================\n",
        "academic = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/·Ñí·Ö°·Ü®·Ñâ·Ö°·Ñã·Öµ·ÜØ·Ñå·Ö•·Üº_·Ñå·Ö•·Üº·ÑÖ·Öµ(2325).csv')\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test  = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5) ÎÇ†Ïßú ÌîºÏ≤ò Ï∂îÍ∞Ä\n",
        "# =========================================================\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "train = add_date_features(train)\n",
        "test  = add_date_features(test)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 6) exog columns (GRUÏôÄ ÎèôÏùº)\n",
        "# =========================================================\n",
        "exog_cols = [\n",
        "    'acad_weekend','acad_semester','acad_weekday',\n",
        "    'acad_holiday','acad_seasonal','acad_exam',\n",
        "    'acad_ceremony','acad_dormitory',\n",
        "    'month','weekday','is_weekend','open_hours'\n",
        "]\n",
        "\n",
        "train[exog_cols] = train[exog_cols].fillna(0)\n",
        "test [exog_cols] = test [exog_cols].fillna(0)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 7) AIC Í∏∞Î∞ò ÏûêÎèô SARIMAX Î™®Îç∏ ÏÑ†ÌÉù\n",
        "# =========================================================\n",
        "best_model = sarimax_auto_search(train, exog_cols)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8) ÏòàÏ∏°\n",
        "# =========================================================\n",
        "test_pred = sarimax_forecast_autoreg(train, test, exog_cols, best_model)\n",
        "\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], test_pred)\n",
        "\n",
        "print(\"====== SARIMAX ÏûêÎèôÌÉêÏÉâ Í≤∞Í≥º ======\")\n",
        "print(\"MAE  :\", mae)\n",
        "print(\"RMSE :\", rmse)\n",
        "print(\"SMAPE:\", smape_val)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 9) CSV Ï†ÄÏû•\n",
        "# =========================================================\n",
        "output = test[['ÏòÅÏóÖÏùºÏûê']].copy()\n",
        "output['ÏòàÏ∏°Îß§Ï∂ú'] = test_pred\n",
        "output.to_csv(\"sarimax_auto_prediction.csv\", index=False)\n",
        "\n",
        "print(\"\\nCSV Ï†ÄÏû• ÏôÑÎ£å ‚Üí sarimax_auto_prediction.csv\")\n"
      ],
      "metadata": {
        "id": "zlsi1ENHZ_xq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99693311-869f-4a60-ead6-e0e3f53197ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ï¥ù ÌÉêÏÉâ Ï°∞Ìï© Ïàò: 64\n",
            "\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 23484.44 ‚Üí (0, 0, 0, 0, 0, 0, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 19090.44 ‚Üí (0, 0, 0, 0, 0, 1, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 19026.92 ‚Üí (0, 0, 0, 0, 1, 1, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 19006.22 ‚Üí (0, 0, 0, 1, 1, 1, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 18990.11 ‚Üí (0, 0, 1, 0, 1, 1, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 18974.29 ‚Üí (0, 0, 1, 1, 1, 1, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 18959.89 ‚Üí (1, 0, 1, 0, 1, 1, 7)\n",
            "üî• ÏÉàÎ°úÏö¥ ÏµúÏ†Å AIC: 18952.35 ‚Üí (1, 0, 1, 1, 1, 1, 7)\n",
            "\n",
            "==========================\n",
            "üìå ÏµúÏ¢Ö ÏÑ†ÌÉùÎêú SARIMAX Î™®Îç∏\n",
            "order = (1, 0, 1)\n",
            "seasonal_order = (1, 1, 1, 7)\n",
            "AIC = 18952.35459120816\n",
            "==========================\n",
            "\n",
            "====== SARIMAX ÏûêÎèôÌÉêÏÉâ Í≤∞Í≥º ======\n",
            "MAE  : 63429.931170126794\n",
            "RMSE : 79420.76312946768\n",
            "SMAPE: 76.60371037543867\n",
            "\n",
            "CSV Ï†ÄÏû• ÏôÑÎ£å ‚Üí sarimax_auto_prediction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRUÏôÄ SARIMAX Î∏îÎ†åÎî©**"
      ],
      "metadata": {
        "id": "V74o-1_6d0LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 0) Seed Í≥†Ï†ï\n",
        "# =========================================================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 1) ÌèâÍ∞Ä ÏßÄÌëú\n",
        "# =========================================================\n",
        "def smape(a, f):\n",
        "    a = np.array(a)\n",
        "    f = np.array(f)\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2\n",
        "    mask = denom != 0\n",
        "    return np.mean(np.abs(a[mask] - f[mask]) / denom[mask]) * 100\n",
        "\n",
        "def evaluate(actual, pred):\n",
        "    actual = np.array(actual)\n",
        "    pred = np.array(pred)\n",
        "    mae = np.mean(np.abs(actual - pred))\n",
        "    rmse = np.sqrt(np.mean((actual - pred)**2))\n",
        "    smape_v = smape(actual, pred)\n",
        "    return mae, rmse, smape_v\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2) Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
        "# =========================================================\n",
        "def clean_sales(df):\n",
        "    df['ÏùºÎß§Ï∂ú'] = (\n",
        "        df['ÏùºÎß§Ï∂ú'].astype(str)\n",
        "        .str.replace(\",\", \"\")\n",
        "        .str.replace(\" \", \"\")\n",
        "        .str.strip()\n",
        "    )\n",
        "    df['ÏùºÎß§Ï∂ú'] = pd.to_numeric(df['ÏùºÎß§Ï∂ú'], errors='coerce').fillna(0)\n",
        "    return df\n",
        "\n",
        "def add_date_features(df):\n",
        "    df['month'] = df['ÏòÅÏóÖÏùºÏûê'].dt.month\n",
        "    df['day'] = df['ÏòÅÏóÖÏùºÏûê'].dt.day\n",
        "    df['weekday'] = df['ÏòÅÏóÖÏùºÏûê'].dt.weekday\n",
        "    df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
        "\n",
        "    df['open_hours'] = 11\n",
        "    df.loc[df['weekday'] == 5, 'open_hours'] = 6\n",
        "    df.loc[df['weekday'] == 6, 'open_hours'] = 0\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df):\n",
        "    df['lag1'] = df['ÏùºÎß§Ï∂ú'].shift(1)\n",
        "    df['lag7'] = df['ÏùºÎß§Ï∂ú'].shift(7)\n",
        "    df['lag14'] = df['ÏùºÎß§Ï∂ú'].shift(14)\n",
        "    df['lag28'] = df['ÏùºÎß§Ï∂ú'].shift(28)\n",
        "\n",
        "    df['roll_mean7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).mean()\n",
        "    df['roll_mean14'] = df['ÏùºÎß§Ï∂ú'].rolling(14).mean()\n",
        "    df['roll_mean28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).mean()\n",
        "\n",
        "    df['roll_std7'] = df['ÏùºÎß§Ï∂ú'].rolling(7).std()\n",
        "    df['roll_std28'] = df['ÏùºÎß§Ï∂ú'].rolling(28).std()\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3) GRU Dataset\n",
        "# =========================================================\n",
        "def create_sequences(df, feature_cols, seq_len=28):\n",
        "    X, y = [], []\n",
        "    values = df[feature_cols].values\n",
        "    targets = df['ÏùºÎß§Ï∂ú'].values\n",
        "    for i in range(seq_len, len(df)):\n",
        "        X.append(values[i-seq_len:i])\n",
        "        y.append(targets[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4) GRU Î™®Îç∏ Ï†ïÏùò\n",
        "# =========================================================\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5) GRU Autoregressive ÏòàÏ∏°\n",
        "# =========================================================\n",
        "def autoregressive_forecast(model, df_train, df_test, seq_len, feature_cols, scaler_X, scaler_y):\n",
        "\n",
        "    history = df_train.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(df_test)):\n",
        "        row = df_test.iloc[i].copy()\n",
        "        last_vals = history['ÏùºÎß§Ï∂ú'].values\n",
        "\n",
        "        row['lag1'] = last_vals[-1]\n",
        "        row['lag7'] = last_vals[-7]\n",
        "        row['lag14'] = last_vals[-14]\n",
        "        row['lag28'] = last_vals[-28]\n",
        "\n",
        "        row['roll_mean7'] = pd.Series(last_vals[-7:]).mean()\n",
        "        row['roll_mean14'] = pd.Series(last_vals[-14:]).mean()\n",
        "        row['roll_mean28'] = pd.Series(last_vals[-28:]).mean()\n",
        "        row['roll_std7'] = pd.Series(last_vals[-7:]).std()\n",
        "        row['roll_std28'] = pd.Series(last_vals[-28:]).std()\n",
        "\n",
        "        seq_df = history.tail(seq_len).copy()\n",
        "        seq_scaled = scaler_X.transform(seq_df[feature_cols])\n",
        "        X = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        pred_scaled = model(X).item()\n",
        "        pred = scaler_y.inverse_transform([[pred_scaled]])[0][0]\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "        row['ÏùºÎß§Ï∂ú'] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 6) SARIMAX ÏòàÏ∏° (exog Í∏∞Î∞ò)\n",
        "# =========================================================\n",
        "def sarimax_forecast(train_df, test_df, exog_cols):\n",
        "\n",
        "    # SARIMAX Í∏∞Î≥∏ ÌååÎùºÎØ∏ÌÑ∞ (ÏöîÏùº Ï£ºÍ∏∞)\n",
        "    order = (1,1,1)\n",
        "    seasonal = (1,1,1,7)\n",
        "\n",
        "    history = train_df.copy()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "\n",
        "        model = SARIMAX(\n",
        "            history[\"ÏùºÎß§Ï∂ú\"],\n",
        "            exog=history[exog_cols],\n",
        "            order=order,\n",
        "            seasonal_order=seasonal,\n",
        "            enforce_stationarity=False,\n",
        "            enforce_invertibility=False\n",
        "        )\n",
        "\n",
        "        fit = model.fit(disp=False)\n",
        "\n",
        "        exog_next = test_df.loc[[i], exog_cols].fillna(0)\n",
        "\n",
        "        forecast = fit.predict(\n",
        "            start=len(history),\n",
        "            end=len(history),\n",
        "            exog=exog_next\n",
        "        )\n",
        "\n",
        "        pred = float(forecast.values[0])\n",
        "        pred = max(pred, 0)\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "        new_row = test_df.iloc[i].copy()\n",
        "        new_row[\"ÏùºÎß§Ï∂ú\"] = pred\n",
        "        history = pd.concat([history, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 7) Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "# =========================================================\n",
        "train = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/POS_train_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/POS_test.csv')\n",
        "\n",
        "train['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(train['ÏòÅÏóÖÏùºÏûê'])\n",
        "test['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(test['ÏòÅÏóÖÏùºÏûê'])\n",
        "\n",
        "train = clean_sales(train)\n",
        "test = clean_sales(test)\n",
        "\n",
        "\n",
        "\n",
        "# ----------------- ÌïôÏÇ¨ÏùºÏ†ï merge -----------------\n",
        "academic = pd.read_csv('/content/drive/MyDrive/·ÑÄ·Öµ·ÑÄ·Ö®·Ñí·Ö°·Ü®·Ñâ·Ö≥·Ü∏/·Ñê·Öµ·Ü∑·Ñë·Ö≥·ÜØ/Data/·Ñí·Ö°·Ü®·Ñâ·Ö°·Ñã·Öµ·ÜØ·Ñå·Ö•·Üº_·Ñå·Ö•·Üº·ÑÖ·Öµ(2325).csv')\n",
        "academic['ÏòÅÏóÖÏùºÏûê'] = pd.to_datetime(academic['date'])\n",
        "academic = academic.drop(columns=['date'])\n",
        "\n",
        "weekday_map = {'mon':0,'tue':1,'wed':2,'thu':3,'fri':4,'sat':5,'sun':6}\n",
        "academic['acad_weekday'] = academic['weekday'].map(weekday_map)\n",
        "academic = academic.drop(columns=['weekday'])\n",
        "\n",
        "academic = academic.rename(columns={\n",
        "    'weekend':'acad_weekend',\n",
        "    'holiday':'acad_holiday',\n",
        "    'semester':'acad_semester',\n",
        "    'seasonal':'acad_seasonal',\n",
        "    'exam':'acad_exam',\n",
        "    'ceremony':'acad_ceremony',\n",
        "    'dormitory':'acad_dormitory'\n",
        "})\n",
        "\n",
        "train = train.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "test = test.merge(academic, on='ÏòÅÏóÖÏùºÏûê', how='left')\n",
        "\n",
        "train = add_date_features(train)\n",
        "test = add_date_features(test)\n",
        "\n",
        "train = add_lag_features(train).dropna().reset_index(drop=True)\n",
        "test = add_lag_features(test)\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8) GRU ÌïôÏäµ\n",
        "# =========================================================\n",
        "feature_cols = [\n",
        "    'acad_weekend','acad_semester','acad_weekday',\n",
        "    'open_hours','acad_ceremony','acad_exam',\n",
        "    'lag1','lag7','lag14','lag28',\n",
        "    'roll_std7','roll_std28',\n",
        "    'roll_mean7','roll_mean14','roll_mean28'\n",
        "]\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "train[feature_cols] = scaler_X.fit_transform(train[feature_cols])\n",
        "train[['ÏùºÎß§Ï∂ú']] = scaler_y.fit_transform(train[['ÏùºÎß§Ï∂ú']])\n",
        "\n",
        "seq_len = 28\n",
        "X_train, y_train = create_sequences(train, feature_cols, seq_len)\n",
        "\n",
        "dataset = SequenceDataset(X_train, y_train)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = GRUModel(input_dim=len(feature_cols), num_layers=2)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Epoch {epoch+1}/50] Loss: {total_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 9) GRU + SARIMAX ÏòàÏ∏° + Blending\n",
        "# =========================================================\n",
        "gru_pred = autoregressive_forecast(model, train, test, seq_len, feature_cols, scaler_X, scaler_y)\n",
        "\n",
        "# SARIMAX exog subset\n",
        "exog_cols = [\n",
        "    'acad_weekend','acad_semester','acad_weekday',\n",
        "    'acad_holiday','acad_seasonal','acad_exam',\n",
        "    'acad_ceremony','acad_dormitory',\n",
        "    'month','weekday','is_weekend','open_hours'\n",
        "]\n",
        "\n",
        "sarimax_pred = sarimax_forecast(train, test, exog_cols)\n",
        "\n",
        "# ---------- Blending ----------\n",
        "alpha = 0.7     # GRU Í∞ÄÏ§ëÏπò\n",
        "final_pred = alpha * np.array(gru_pred) + (1-alpha) * np.array(sarimax_pred)\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 10) ÌèâÍ∞Ä\n",
        "# =========================================================\n",
        "mae, rmse, smape_val = evaluate(test['ÏùºÎß§Ï∂ú'], final_pred)\n",
        "print(\"\\n====== GRU + SARIMAX Blending Í≤∞Í≥º ======\")\n",
        "print(\"MAE   :\", mae)\n",
        "print(\"RMSE  :\", rmse)\n",
        "print(\"SMAPE :\", smape_val)\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 11) CSV Ï†ÄÏû•\n",
        "# =========================================================\n",
        "output = test[['ÏòÅÏóÖÏùºÏûê']].copy()\n",
        "output['GRU'] = gru_pred\n",
        "output['SARIMAX'] = sarimax_pred\n",
        "output['Blended'] = final_pred\n",
        "\n",
        "output.to_csv(\"gru_sarimax_blended.csv\", index=False)\n",
        "print(\"\\nCSV Ï†ÄÏû• ÏôÑÎ£å ‚Üí gru_sarimax_blended.csv\")\n"
      ],
      "metadata": {
        "id": "nKvTGdS4eFcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5764e69-fd50-4096-c9f4-187e28bd41fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] Loss: 0.8714\n",
            "[Epoch 2/50] Loss: 0.5253\n",
            "[Epoch 3/50] Loss: 0.4432\n",
            "[Epoch 4/50] Loss: 0.3666\n",
            "[Epoch 5/50] Loss: 0.3487\n",
            "[Epoch 6/50] Loss: 0.3307\n",
            "[Epoch 7/50] Loss: 0.3320\n",
            "[Epoch 8/50] Loss: 0.3193\n",
            "[Epoch 9/50] Loss: 0.3023\n",
            "[Epoch 10/50] Loss: 0.2984\n",
            "[Epoch 11/50] Loss: 0.2922\n",
            "[Epoch 12/50] Loss: 0.2591\n",
            "[Epoch 13/50] Loss: 0.2478\n",
            "[Epoch 14/50] Loss: 0.2435\n",
            "[Epoch 15/50] Loss: 0.2300\n",
            "[Epoch 16/50] Loss: 0.2335\n",
            "[Epoch 17/50] Loss: 0.2289\n",
            "[Epoch 18/50] Loss: 0.2137\n",
            "[Epoch 19/50] Loss: 0.2119\n",
            "[Epoch 20/50] Loss: 0.2062\n",
            "[Epoch 21/50] Loss: 0.2119\n",
            "[Epoch 22/50] Loss: 0.2106\n",
            "[Epoch 23/50] Loss: 0.2130\n",
            "[Epoch 24/50] Loss: 0.2027\n",
            "[Epoch 25/50] Loss: 0.2083\n",
            "[Epoch 26/50] Loss: 0.2082\n",
            "[Epoch 27/50] Loss: 0.2012\n",
            "[Epoch 28/50] Loss: 0.1947\n",
            "[Epoch 29/50] Loss: 0.2009\n",
            "[Epoch 30/50] Loss: 0.1999\n",
            "[Epoch 31/50] Loss: 0.2094\n",
            "[Epoch 32/50] Loss: 0.2004\n",
            "[Epoch 33/50] Loss: 0.1923\n",
            "[Epoch 34/50] Loss: 0.1984\n",
            "[Epoch 35/50] Loss: 0.1918\n",
            "[Epoch 36/50] Loss: 0.2006\n",
            "[Epoch 37/50] Loss: 0.1884\n",
            "[Epoch 38/50] Loss: 0.1917\n",
            "[Epoch 39/50] Loss: 0.1866\n",
            "[Epoch 40/50] Loss: 0.1959\n",
            "[Epoch 41/50] Loss: 0.1977\n",
            "[Epoch 42/50] Loss: 0.1912\n",
            "[Epoch 43/50] Loss: 0.1850\n",
            "[Epoch 44/50] Loss: 0.1799\n",
            "[Epoch 45/50] Loss: 0.1870\n",
            "[Epoch 46/50] Loss: 0.1854\n",
            "[Epoch 47/50] Loss: 0.1791\n",
            "[Epoch 48/50] Loss: 0.1936\n",
            "[Epoch 49/50] Loss: 0.2071\n",
            "[Epoch 50/50] Loss: 0.2062\n",
            "\n",
            "====== GRU + SARIMAX Blending Í≤∞Í≥º ======\n",
            "MAE   : 75877.17634897862\n",
            "RMSE  : 102501.74264765943\n",
            "SMAPE : 97.05220874758207\n",
            "\n",
            "CSV Ï†ÄÏû• ÏôÑÎ£å ‚Üí gru_sarimax_blended.csv\n"
          ]
        }
      ]
    }
  ]
}