# -*- coding: utf-8 -*-
"""LGBM_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1npStpaTFAOu_UHDrL1Y8pNG70YHu-wR7
"""

import pandas as pd
import numpy as np
import random
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import mean_absolute_error, mean_squared_error
import lightgbm as lgb

# =========================
# 0. 전역 시드 고정 (재현성 확보)
# =========================
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

FEATURE_PATH = "/content/drive/MyDrive/기계학습/팀플/Data/학사일정_정리(2325).csv"
TRAIN_PATH   = "/content/drive/MyDrive/기계학습/팀플/Data/ENG_POS_train_val.csv"
TEST_PATH = "/content/drive/MyDrive/기계학습/팀플/Data/ENG_POS_test.csv"


# =========================
# 1. 유틸 함수들
# =========================
def smape(y_true, y_pred):
    y_true = np.asarray(y_true, float)
    y_pred = np.asarray(y_pred, float)
    return np.mean(
        2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)
    ) * 100


def calculate_operating_hours(row):
    weekday = row["weekday"]
    semester = row["semester"]
    holiday = row["holiday"]

    # 일요일 미운영
    if weekday == "sun":
        return 0

    # 학기 중
    if semester == 1:
        if holiday == 1:
            return 7
        if weekday in ["mon", "tue", "wed", "thu", "fri"]:
            return 12
        if weekday == "sat":
            return 7
        return 0

    # 방학 중
    else:
        if holiday == 1:
            return 0
        if weekday in ["mon", "tue", "wed", "thu", "fri", "sat"]:
            return 7
        return 0


# =========================
# 2. 메인 파이프라인
# =========================
def run_lgbm(feature_path=FEATURE_PATH, train_path=TRAIN_PATH, test_path=TEST_PATH):

    # ---------- (1) 데이터 로드 ----------
    feat = pd.read_csv(feature_path)
    pos_train = pd.read_csv(train_path)
    pos_test = pd.read_csv(test_path)

    # 숫자 전처리
    for df in [pos_train, pos_test]:
        for col in ["daily", "AOV"]:
            df[col] = (
                df[col].astype(str).str.replace(",", "", regex=False).astype(float)
            )
        df["date"] = pd.to_datetime(df["date"])

    feat["date"] = pd.to_datetime(feat["date"])

    # train/test 결합
    pos_train["set"] = "train"
    pos_test["set"] = "test"

    # 전체 병합 (Static Feature 계산용)
    pos_all = pd.concat([pos_train, pos_test], ignore_index=True)
    df_all = pd.merge(pos_all, feat, on="date", how="left")
    df_all = df_all.sort_values("date").reset_index(drop=True)

    print("All merged shape:", df_all.shape)

    # ---------- (2) Static Feature Engineering ----------
    df_all["operating_hours"] = df_all.apply(calculate_operating_hours, axis=1)

    # [New Feature] Month (계절성 반영)
    df_all["month"] = df_all["date"].dt.month

    df_all["exam_before3"] = df_all["exam"].shift(1).rolling(3, min_periods=1).sum().fillna(0)
    df_all["exam_after3"] = df_all["exam"].shift(-1).rolling(3, min_periods=1).sum().fillna(0)

    df_all["semester_weekend"] = df_all["semester"] * df_all["weekend"]

    df_all = pd.get_dummies(df_all, columns=["weekday"], drop_first=True)

    # ---------- (3) Train/Test 분리 ----------
    df_train_val = df_all[df_all["set"] == "train"].copy()
    df_test_static = df_all[df_all["set"] == "test"].copy()

    print("\nGenerating features for Training set...")
    lag_list = [1, 2, 3, 7, 14, 28]
    win_list = [7, 14, 28]

    for lag in lag_list:
        df_train_val[f"Lag{lag}"] = df_train_val["daily"].shift(lag)

    for win in win_list:
        roll = df_train_val["daily"].rolling(window=win, min_periods=1)
        df_train_val[f"RollingMean{win}"] = roll.mean().shift(1)
        df_train_val[f"RollingStd{win}"] = roll.std(ddof=0).shift(1)

    feature_na_cols = [f"Lag{l}" for l in lag_list] + \
                      [f"RollingMean{w}" for w in win_list] + \
                      [f"RollingStd{w}" for w in win_list]

    df_train_val_clean = df_train_val.dropna(subset=feature_na_cols).reset_index(drop=True)
    df_train_val_clean = df_train_val_clean[df_train_val_clean["daily"] > 0].copy()

    print("Train_val prepared:", df_train_val_clean.shape)

    # ---------- (4) Train/Val Split & Model Training (Log Transform) ----------
    df_sorted = df_train_val_clean.sort_values("date").reset_index(drop=True)
    split_idx = int(len(df_sorted) * 0.8)

    train = df_sorted.iloc[:split_idx]
    val = df_sorted.iloc[split_idx:]

    drop_cols = ["date", "daily", "num", "AOV", "set"]
    feature_cols = [c for c in df_sorted.columns if c not in drop_cols]

    X_train = train[feature_cols]
    y_train = np.log1p(train["daily"]) # Log Transform
    X_val = val[feature_cols]
    y_val = np.log1p(val["daily"])     # Log Transform

    print(f"Training with {len(feature_cols)} features (Target Log-Transformed)...")

    from itertools import product
    param_grid = {
        "num_leaves":       [15, 31],
        "learning_rate":    [0.03, 0.05],
        "n_estimators":     [400, 800],
        "min_child_samples":[20],
        "subsample":        [0.8],
        "colsample_bytree": [0.8],
    }

    best_rmse = np.inf
    best_params = None

    for num_leaves, lr, n_estimators, min_child, subsample, colsample in product(
        param_grid["num_leaves"], param_grid["learning_rate"], param_grid["n_estimators"],
        param_grid["min_child_samples"], param_grid["subsample"], param_grid["colsample_bytree"],
    ):
        params = {
            "num_leaves": num_leaves, "learning_rate": lr, "n_estimators": n_estimators,
            "min_child_samples": min_child, "subsample": subsample, "colsample_bytree": colsample,
        }
        reg_tmp = lgb.LGBMRegressor(
            objective="regression", random_state=SEED, verbosity=-1, **params
        )
        reg_tmp.fit(X_train, y_train)
        val_pred_tmp = reg_tmp.predict(X_val)
        rmse_tmp = np.sqrt(mean_squared_error(y_val, val_pred_tmp))

        if rmse_tmp < best_rmse:
            best_rmse = rmse_tmp
            best_params = params

    print(f"Best Params: {best_params}")

    # Final Fit
    reg = lgb.LGBMRegressor(objective="regression", random_state=SEED, verbosity=-1, **best_params)
    reg.fit(df_sorted[feature_cols], np.log1p(df_sorted["daily"])) # Final Fit with Log

    # ---------- (5) Recursive Forecasting for Test (Black Box Simulation) ----------
    print("\nStarting Recursive Forecasting for Test Set...")

    history_df = df_train_val.iloc[-60:].copy()
    test_preds = []

    for idx, row in df_test_static.iterrows():
        current_row = row.to_frame().T
        current_row["daily"] = np.nan

        temp_history = pd.concat([history_df, current_row], ignore_index=True)

        for lag in lag_list:
            temp_history[f"Lag{lag}"] = temp_history["daily"].shift(lag)

        for win in win_list:
            roll = temp_history["daily"].rolling(window=win, min_periods=1)
            temp_history[f"RollingMean{win}"] = roll.mean().shift(1)
            temp_history[f"RollingStd{win}"] = roll.std(ddof=0).shift(1)

        X_test_single = temp_history.iloc[[-1]][feature_cols]
        X_test_single = X_test_single.astype(float)

        # Predict Log Value
        pred_log = reg.predict(X_test_single)[0]

        # Inverse Transform (Exp)
        pred = np.expm1(pred_log)

        test_preds.append(pred)

        current_row["daily"] = pred
        history_df = pd.concat([history_df, current_row], ignore_index=True).iloc[-60:]

    # ---------- (6) 성능 평가 ----------
    # 실제값 가져오기 (평가용으로만 사용)
    y_test_actual = df_test_static["daily"].values

    # 0인 날짜 제외하고 평가 (기존 로직 따름)
    # 단, 예측은 모든 날짜에 대해 수행했으므로 인덱싱으로 필터링
    mask = y_test_actual > 0

    final_actual = y_test_actual[mask]
    final_pred = np.array(test_preds)[mask]

    mae_test = mean_absolute_error(final_actual, final_pred)
    rmse_test = np.sqrt(mean_squared_error(final_actual, final_pred))
    smape_test = smape(final_actual, final_pred)

    print("\n===== Test Performance (Recursive / Black-box) =====")
    print(f"MAE   : {mae_test:,.2f}")
    print(f"RMSE  : {rmse_test:,.2f}")
    print(f"SMAPE : {smape_test:.2f}%")

    # 결과 저장
    result_df = df_test_static.copy()
    result_df["pred_daily"] = test_preds
    result_df = result_df[result_df["daily"] > 0] # 평가 기준 동일하게 저장

    result_df = result_df[["date", "daily", "pred_daily"]].rename(columns={"daily": "actual_daily"})
    result_df.to_csv("lgbm_prediction_recursive.csv", index=False, encoding="utf-8-sig")

    metrics = {
        "model": "LGBM_Recursive",
        "test_MAE": mae_test,
        "test_RMSE": rmse_test,
        "test_SMAPE": smape_test,
    }

    return metrics, result_df

if __name__ == "__main__":
    metrics, df_pred = run_lgbm()
    print("\n[DEBUG] Metrics:", metrics)
    print(df_pred.head())