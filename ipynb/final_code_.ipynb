{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import gc\n",
        "import random\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 0. Seed ê³ ì • (ì¬í˜„ì„± ë³´ì¥)\n",
        "# ============================================\n",
        "SEED = 42\n",
        "\n",
        "# Python hash seed\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Python random\n",
        "random.seed(SEED)\n",
        "\n",
        "# NumPy\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# PyTorch\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ì¶”ê°€ì ì¸ ì¬í˜„ì„± ë³´ì¥\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # CUDA ì¬í˜„ì„±\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ============================================\n",
        "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "# ============================================\n",
        "def clean_numeric(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == \"object\":\n",
        "            df[col] = df[col].astype(str).str.replace(\",\", \"\")\n",
        "            try:\n",
        "                df[col] = pd.to_numeric(df[col])\n",
        "            except (ValueError, TypeError):\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, min_delta=0.0001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.best_model_state = model.state_dict().copy()\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n",
        "\n",
        "    def load_best_model(self, model):\n",
        "        if self.best_model_state is not None:\n",
        "            model.load_state_dict(self.best_model_state)\n",
        "\n",
        "# ============================================\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "# ============================================\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/Colab Notebooks/POS_train_val.csv\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/Colab Notebooks/POS_test.csv\"\n",
        "ACAD_PATH = \"/content/drive/MyDrive/Colab Notebooks/í•™ì‚¬ì¼ì •_ì •ë¦¬(2325).csv\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "acad = pd.read_csv(ACAD_PATH)\n",
        "\n",
        "train[\"ì˜ì—…ì¼ì\"] = pd.to_datetime(train[\"ì˜ì—…ì¼ì\"])\n",
        "test[\"ì˜ì—…ì¼ì\"] = pd.to_datetime(test[\"ì˜ì—…ì¼ì\"])\n",
        "acad[\"ì˜ì—…ì¼ì\"] = pd.to_datetime(acad[\"date\"])\n",
        "acad_cols = [c for c in acad.columns if c != \"ì˜ì—…ì¼ì\" and c != \"date\"]\n",
        "acad = acad.drop(columns=[\"date\"])\n",
        "\n",
        "train = train.merge(acad, on=\"ì˜ì—…ì¼ì\", how=\"left\")\n",
        "test = test.merge(acad, on=\"ì˜ì—…ì¼ì\", how=\"left\")\n",
        "\n",
        "train = train.sort_values(\"ì˜ì—…ì¼ì\").reset_index(drop=True)\n",
        "test = test.sort_values(\"ì˜ì—…ì¼ì\").reset_index(drop=True)\n",
        "\n",
        "print(f\"\\ní•™ì‚¬ì¼ì • ë°ì´í„° ì»¬ëŸ¼: {acad_cols}\")\n",
        "\n",
        "# ì¼ë§¤ì¶œ ì •ë¦¬\n",
        "train[\"ì¼ë§¤ì¶œ\"] = train[\"ì¼ë§¤ì¶œ\"].astype(str).str.replace(\",\", \"\").str.replace(\" \", \"\")\n",
        "train = clean_numeric(train)\n",
        "train[\"ì¼ë§¤ì¶œ\"] = pd.to_numeric(train[\"ì¼ë§¤ì¶œ\"], errors='coerce').fillna(0)\n",
        "\n",
        "if \"ì¼ë§¤ì¶œ\" in test.columns:\n",
        "    test[\"ì¼ë§¤ì¶œ\"] = test[\"ì¼ë§¤ì¶œ\"].astype(str).str.replace(\",\", \"\").str.replace(\" \", \"\")\n",
        "    test = clean_numeric(test)\n",
        "    test[\"ì¼ë§¤ì¶œ\"] = pd.to_numeric(test[\"ì¼ë§¤ì¶œ\"], errors='coerce').fillna(0)\n",
        "\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# PART 1: ì²« ë²ˆì§¸ ì½”ë“œ - LSTM (ê·¸ëŒ€ë¡œ ì´ì–´ë¶™ì´ê¸°)\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PART 1: LSTM ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ì²« ë²ˆì§¸ ì½”ë“œ ê·¸ëŒ€ë¡œ)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --------------------------------------------\n",
        "# ì²« ë²ˆì§¸ ì½”ë“œ: Feature Engineering\n",
        "# --------------------------------------------\n",
        "def make_basic_features_lstm(df):\n",
        "    \"\"\"ì‹œê°„ ì •ë³´ë§Œ ì¶”ê°€ (ëˆ„ìˆ˜ ì—†ëŠ” feature)\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"DayOfWeek\"] = df[\"ì˜ì—…ì¼ì\"].dt.dayofweek\n",
        "    df[\"Month\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
        "    df[\"Day\"] = df[\"ì˜ì—…ì¼ì\"].dt.day\n",
        "    df[\"IsWeekend\"] = (df[\"DayOfWeek\"] >= 5).astype(int)\n",
        "    df[\"OpHoursFactor\"] = df[\"DayOfWeek\"].apply(lambda x: 1 if x<5 else (0.55 if x==5 else 0))\n",
        "    return df\n",
        "\n",
        "def make_features_lstm(df):\n",
        "    \"\"\"ì „ì²´ feature ìƒì„±\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"DayOfWeek\"] = df[\"ì˜ì—…ì¼ì\"].dt.dayofweek\n",
        "    df[\"Month\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
        "    df[\"Day\"] = df[\"ì˜ì—…ì¼ì\"].dt.day\n",
        "    df[\"IsWeekend\"] = (df[\"DayOfWeek\"] >= 5).astype(int)\n",
        "    df[\"OpHoursFactor\"] = df[\"DayOfWeek\"].apply(lambda x: 1 if x<5 else (0.55 if x==5 else 0))\n",
        "\n",
        "    # 0ì¸ ë‚ (íœ´ë¬´ì¼) ê°„ë‹¨í•œ binary featureë§Œ ì¶”ê°€\n",
        "    df[\"IsZeroSales\"] = (df[\"ì¼ë§¤ì¶œ\"] == 0).astype(int)\n",
        "\n",
        "    for lag in [1,2,3,7,14,28]:\n",
        "        df[f\"Lag{lag}\"] = df[\"ì¼ë§¤ì¶œ\"].shift(lag)\n",
        "    for w in [7,14,28]:\n",
        "        df[f\"Mean{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).mean()\n",
        "        df[f\"Std{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).std()\n",
        "    return df.fillna(0)\n",
        "\n",
        "train_lstm = make_features_lstm(train.copy())\n",
        "\n",
        "# Train/Validation Split\n",
        "train_df_lstm, val_df_lstm = train_test_split(train_lstm, test_size=0.2, shuffle=False, random_state=SEED)\n",
        "\n",
        "# Feature Selection\n",
        "numeric_cols_lstm = train_df_lstm.select_dtypes(include=[np.number]).columns.tolist()\n",
        "string_cols_lstm = train_df_lstm.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "meta_cols_lstm = [\n",
        "    c for c in numeric_cols_lstm\n",
        "    if c not in [\"ì¼ë§¤ì¶œ\", \"sales_scaled\"]\n",
        "    and not c.startswith(\"Lag\")\n",
        "    and not c.startswith(\"Mean\")\n",
        "    and not c.startswith(\"Std\")\n",
        "]\n",
        "meta_cols_lstm = [c for c in meta_cols_lstm if c not in string_cols_lstm]\n",
        "\n",
        "tree_features_lstm = meta_cols_lstm + [c for c in train_df_lstm.columns\n",
        "                                      if (c.startswith(\"Lag\") or c.startswith(\"Mean\") or c.startswith(\"Std\"))]\n",
        "tree_features_lstm = [c for c in tree_features_lstm if c not in string_cols_lstm]\n",
        "\n",
        "# Scaling\n",
        "sales_scaler_lstm = MinMaxScaler()\n",
        "meta_scaler_lstm = MinMaxScaler()\n",
        "\n",
        "train_df_lstm[\"sales_scaled\"] = sales_scaler_lstm.fit_transform(train_df_lstm[[\"ì¼ë§¤ì¶œ\"]])\n",
        "train_df_lstm[meta_cols_lstm] = meta_scaler_lstm.fit_transform(train_df_lstm[meta_cols_lstm])\n",
        "\n",
        "val_df_lstm[\"sales_scaled\"] = sales_scaler_lstm.transform(val_df_lstm[[\"ì¼ë§¤ì¶œ\"]])\n",
        "val_df_lstm[meta_cols_lstm] = meta_scaler_lstm.transform(val_df_lstm[meta_cols_lstm])\n",
        "\n",
        "# Sequence Dataset\n",
        "LOOKBACK_LSTM = 21\n",
        "BATCH_LSTM = 16\n",
        "EPOCHS_LSTM = 100\n",
        "\n",
        "def create_seq_lstm(df, lookback):\n",
        "    X, y = [], []\n",
        "    sales = df[\"sales_scaled\"].values\n",
        "    meta = df[meta_cols_lstm].values\n",
        "    for i in range(len(df)-lookback):\n",
        "        seq = np.concatenate([\n",
        "            sales[i:i+lookback].reshape(lookback,1),\n",
        "            meta[i:i+lookback]\n",
        "        ], axis=1)\n",
        "        X.append(seq)\n",
        "        y.append(sales[i+lookback])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train_lstm, y_train_lstm = create_seq_lstm(train_df_lstm, LOOKBACK_LSTM)\n",
        "X_val_lstm, y_val_lstm = create_seq_lstm(val_df_lstm, LOOKBACK_LSTM)\n",
        "\n",
        "X_train_lstm = torch.tensor(X_train_lstm, dtype=torch.float32).to(DEVICE)\n",
        "y_train_lstm = torch.tensor(y_train_lstm, dtype=torch.float32).to(DEVICE)\n",
        "X_val_lstm = torch.tensor(X_val_lstm, dtype=torch.float32).to(DEVICE)\n",
        "y_val_lstm = torch.tensor(y_val_lstm, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Models\n",
        "input_dim_lstm = 1 + len(meta_cols_lstm)\n",
        "\n",
        "class MetaLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=128, num_layers=2, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.fc1 = nn.Linear(hidden, hidden // 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden // 2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1]\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        return self.fc2(out)\n",
        "\n",
        "lstm_model = MetaLSTM(input_dim_lstm, hidden=128, num_layers=2, dropout=0.4).to(DEVICE)\n",
        "opt_lstm = torch.optim.AdamW(lstm_model.parameters(), lr=0.0005, weight_decay=0)\n",
        "scheduler_lstm = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_lstm, mode='min', factor=0.5, patience=5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# DataLoaderì— ì‹œë“œ ê³ ì •\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = SEED % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "generator_lstm = torch.Generator()\n",
        "generator_lstm.manual_seed(SEED)\n",
        "\n",
        "train_dataset_lstm = TensorDataset(X_train_lstm, y_train_lstm)\n",
        "train_loader_lstm = DataLoader(train_dataset_lstm, batch_size=BATCH_LSTM, shuffle=True, generator=generator_lstm, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train\n",
        "early_stop_lstm = EarlyStopping(patience=15, min_delta=0.0001)\n",
        "print(\"=\" * 50)\n",
        "print(\"LSTM í•™ìŠµ ì‹œì‘\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ep in range(EPOCHS_LSTM):\n",
        "    lstm_model.train()\n",
        "    train_loss_lstm = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader_lstm:\n",
        "        opt_lstm.zero_grad()\n",
        "        pred_l = lstm_model(batch_x).squeeze()\n",
        "        loss_l = criterion(pred_l, batch_y)\n",
        "        loss_l.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)\n",
        "        opt_lstm.step()\n",
        "        train_loss_lstm += loss_l.item()\n",
        "\n",
        "    lstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred_l = lstm_model(X_val_lstm).squeeze()\n",
        "        val_loss_lstm = criterion(val_pred_l, y_val_lstm).item()\n",
        "\n",
        "    scheduler_lstm.step(val_loss_lstm)\n",
        "\n",
        "    if (ep + 1) % 10 == 0 or ep == 0:\n",
        "        print(f\"[Epoch {ep+1}/{EPOCHS_LSTM}] LSTM - Val: {val_loss_lstm:.6f}\")\n",
        "\n",
        "    if early_stop_lstm(val_loss_lstm, lstm_model):\n",
        "        print(f\"\\nâœ… LSTM Early Stopping at epoch {ep+1}!\")\n",
        "        early_stop_lstm.load_best_model(lstm_model)\n",
        "        break\n",
        "\n",
        "early_stop_lstm.load_best_model(lstm_model)\n",
        "print(\"âœ… LSTM í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# PART B: LSTM ì˜ˆì¸¡\n",
        "train_full_lstm = make_features_lstm(train.copy())\n",
        "train_full_scaled_lstm = train_full_lstm.copy()\n",
        "train_full_scaled_lstm[\"sales_scaled\"] = sales_scaler_lstm.transform(train_full_lstm[[\"ì¼ë§¤ì¶œ\"]])\n",
        "train_full_scaled_lstm[meta_cols_lstm] = meta_scaler_lstm.transform(train_full_lstm[meta_cols_lstm])\n",
        "\n",
        "future_dates = pd.date_range(start=test[\"ì˜ì—…ì¼ì\"].min(), end=test[\"ì˜ì—…ì¼ì\"].max())\n",
        "future_df_lstm = pd.DataFrame({\"ì˜ì—…ì¼ì\": future_dates})\n",
        "future_df_lstm = future_df_lstm.merge(acad, on=\"ì˜ì—…ì¼ì\", how=\"left\")\n",
        "\n",
        "for col in meta_cols_lstm:\n",
        "    if col not in future_df_lstm.columns:\n",
        "        future_df_lstm[col] = 0\n",
        "\n",
        "future_meta_lstm = meta_scaler_lstm.transform(future_df_lstm[meta_cols_lstm])\n",
        "future_meta_lstm = pd.DataFrame(future_meta_lstm, columns=meta_cols_lstm)\n",
        "\n",
        "def predict_nn_autoreg_lstm(model, train_df, future_meta_df, lookback):\n",
        "    \"\"\"ì²« ë²ˆì§¸ ì½”ë“œ: LSTMìš© autoregressive ì˜ˆì¸¡\"\"\"\n",
        "    model.eval()\n",
        "    history_sales = list(train_df[\"sales_scaled\"].values)\n",
        "    history_meta = train_df[meta_cols_lstm].values.tolist()\n",
        "    preds_raw = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(future_meta_df)):\n",
        "            seq_sales = np.array(history_sales[-lookback:]).reshape(lookback, 1)\n",
        "            seq_meta = np.array(history_meta[-lookback:])\n",
        "            seq = np.concatenate([seq_sales, seq_meta], axis=1)\n",
        "\n",
        "            X = torch.tensor(seq, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "            scaled_pred = model(X).item()\n",
        "            raw_pred = sales_scaler_lstm.inverse_transform([[scaled_pred]])[0, 0]\n",
        "\n",
        "            preds_raw.append(raw_pred)\n",
        "\n",
        "            next_scaled = sales_scaler_lstm.transform([[raw_pred]])[0, 0]\n",
        "            history_sales.append(next_scaled)\n",
        "            history_meta.append(future_meta_df.iloc[i].values.tolist())\n",
        "\n",
        "    return np.array(preds_raw)\n",
        "\n",
        "future_lstm_raw = predict_nn_autoreg_lstm(lstm_model, train_full_scaled_lstm, future_meta_lstm, LOOKBACK_LSTM)\n",
        "\n",
        "# ì²« ë²ˆì§¸ ì½”ë“œ: í›„ì²˜ë¦¬\n",
        "def postprocess_zero_days_lstm(preds, future_dates, train_df, threshold_ratio=0.7, small_pred_threshold=10000):\n",
        "    \"\"\"ì²« ë²ˆì§¸ ì½”ë“œ: 0ì¸ ë‚  íŒ¨í„´ í•™ìŠµ ë° ì ìš©\"\"\"\n",
        "    preds = preds.copy()\n",
        "    train_copy = train_df.copy()\n",
        "\n",
        "    train_copy[\"MonthDay\"] = train_copy[\"ì˜ì—…ì¼ì\"].dt.strftime(\"%m-%d\")\n",
        "    train_copy[\"IsZero\"] = (train_copy[\"ì¼ë§¤ì¶œ\"] == 0).astype(int)\n",
        "    zero_ratio_by_date = train_copy.groupby(\"MonthDay\")[\"IsZero\"].mean()\n",
        "\n",
        "    for i, date in enumerate(future_dates):\n",
        "        month_day = date.strftime(\"%m-%d\")\n",
        "        if month_day in zero_ratio_by_date.index:\n",
        "            zero_ratio = zero_ratio_by_date[month_day]\n",
        "            if zero_ratio >= threshold_ratio and preds[i] < small_pred_threshold:\n",
        "                preds[i] = 0\n",
        "    return preds\n",
        "\n",
        "future_lstm_post = postprocess_zero_days_lstm(future_lstm_raw, future_dates, train_full_lstm)\n",
        "future_lstm = future_lstm_post\n",
        "print(f\"LSTM ì˜ˆì¸¡ ì™„ë£Œ - ë²”ìœ„: {future_lstm.min():.2f} ~ {future_lstm.max():.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# PART 2: ë‘ ë²ˆì§¸ ì½”ë“œ - GRU (ë°ì´í„° ëˆ„ìˆ˜ ìˆ˜ì •)\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PART 2: GRU ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ë°ì´í„° ëˆ„ìˆ˜ ìˆ˜ì •)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ë‘ ë²ˆì§¸ ì½”ë“œì˜ Feature Engineering\n",
        "def make_features_gru(df):\n",
        "    \"\"\"ë‘ ë²ˆì§¸ ì½”ë“œ: ì „ì²´ feature ìƒì„±\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"DayOfWeek\"] = df[\"ì˜ì—…ì¼ì\"].dt.dayofweek\n",
        "    df[\"Month\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
        "    df[\"Day\"] = df[\"ì˜ì—…ì¼ì\"].dt.day\n",
        "    df[\"IsWeekend\"] = (df[\"DayOfWeek\"] >= 5).astype(int)\n",
        "    df[\"OpHoursFactor\"] = df[\"DayOfWeek\"].apply(lambda x: 1 if x<5 else (0.55 if x==5 else 0))\n",
        "    df[\"IsZeroSales\"] = (df[\"ì¼ë§¤ì¶œ\"] == 0).astype(int)\n",
        "\n",
        "    for lag in [1,2,3,7,14,28]:\n",
        "        df[f\"Lag{lag}\"] = df[\"ì¼ë§¤ì¶œ\"].shift(lag)\n",
        "    for w in [7,14,28]:\n",
        "        df[f\"Mean{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).mean()\n",
        "        df[f\"Std{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).std()\n",
        "\n",
        "    df[\"Mean3\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(3).mean()\n",
        "    df[\"Max7\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(7).max()\n",
        "    df[\"Min7\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(7).min()\n",
        "    df[\"CV7\"] = df[\"Std7\"] / (df[\"Mean7\"] + 1e-8)\n",
        "\n",
        "    if len(df) > 0:\n",
        "        month_avg = df.groupby(\"Month\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "        df[\"MonthAvg\"] = df[\"Month\"].map(month_avg)\n",
        "        weekday_avg = df.groupby(\"DayOfWeek\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "        df[\"WeekdayAvg\"] = df[\"DayOfWeek\"].map(weekday_avg)\n",
        "\n",
        "    return df.fillna(0)\n",
        "\n",
        "# âœ… ìˆ˜ì •: trainì˜ í‰ê· ê°’ì„ ë¯¸ë¦¬ ê³„ì‚° (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
        "def make_features_gru_safe(df, month_avg_train=None, weekday_avg_train=None):\n",
        "    \"\"\"ë°ì´í„° ëˆ„ìˆ˜ ì—†ëŠ” GRU feature ìƒì„± (trainì˜ í‰ê· ê°’ ì‚¬ìš©)\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"DayOfWeek\"] = df[\"ì˜ì—…ì¼ì\"].dt.dayofweek\n",
        "    df[\"Month\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
        "    df[\"Day\"] = df[\"ì˜ì—…ì¼ì\"].dt.day\n",
        "    df[\"IsWeekend\"] = (df[\"DayOfWeek\"] >= 5).astype(int)\n",
        "    df[\"OpHoursFactor\"] = df[\"DayOfWeek\"].apply(lambda x: 1 if x<5 else (0.55 if x==5 else 0))\n",
        "    df[\"IsZeroSales\"] = (df[\"ì¼ë§¤ì¶œ\"] == 0).astype(int)\n",
        "\n",
        "    for lag in [1,2,3,7,14,28]:\n",
        "        df[f\"Lag{lag}\"] = df[\"ì¼ë§¤ì¶œ\"].shift(lag)\n",
        "    for w in [7,14,28]:\n",
        "        df[f\"Mean{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).mean()\n",
        "        df[f\"Std{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).std()\n",
        "\n",
        "    df[\"Mean3\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(3).mean()\n",
        "    df[\"Max7\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(7).max()\n",
        "    df[\"Min7\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(7).min()\n",
        "    df[\"CV7\"] = df[\"Std7\"] / (df[\"Mean7\"] + 1e-8)\n",
        "\n",
        "    # âœ… trainì˜ í‰ê· ê°’ ì‚¬ìš© (test ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
        "    if month_avg_train is not None:\n",
        "        df[\"MonthAvg\"] = df[\"Month\"].map(month_avg_train).fillna(0)\n",
        "    else:\n",
        "        if len(df) > 0:\n",
        "            month_avg = df.groupby(\"Month\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "            df[\"MonthAvg\"] = df[\"Month\"].map(month_avg).fillna(0)\n",
        "\n",
        "    if weekday_avg_train is not None:\n",
        "        df[\"WeekdayAvg\"] = df[\"DayOfWeek\"].map(weekday_avg_train).fillna(0)\n",
        "    else:\n",
        "        if len(df) > 0:\n",
        "            weekday_avg = df.groupby(\"DayOfWeek\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "            df[\"WeekdayAvg\"] = df[\"DayOfWeek\"].map(weekday_avg).fillna(0)\n",
        "\n",
        "    return df.fillna(0)\n",
        "\n",
        "train_gru = make_features_gru(train.copy())\n",
        "train_df_gru, val_df_gru = train_test_split(train_gru, test_size=0.2, shuffle=False, random_state=SEED)\n",
        "\n",
        "# Feature Selection\n",
        "numeric_cols_gru = train_df_gru.select_dtypes(include=[np.number]).columns.tolist()\n",
        "string_cols_gru = train_df_gru.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "meta_cols_gru = [\n",
        "    c for c in numeric_cols_gru\n",
        "    if c not in [\"ì¼ë§¤ì¶œ\", \"sales_scaled\"]\n",
        "    and not c.startswith(\"Lag\")\n",
        "    and not c.startswith(\"Mean\")\n",
        "    and not c.startswith(\"Std\")\n",
        "    and not c.startswith(\"Max\")\n",
        "    and not c.startswith(\"Min\")\n",
        "    and not c.startswith(\"CV\")\n",
        "    and not c.startswith(\"MonthAvg\")\n",
        "    and not c.startswith(\"WeekdayAvg\")\n",
        "    and c not in acad_cols\n",
        "]\n",
        "meta_cols_gru = [c for c in meta_cols_gru if c not in string_cols_gru]\n",
        "\n",
        "tree_features_gru = meta_cols_gru + [c for c in train_df_gru.columns\n",
        "                                     if (c.startswith(\"Lag\") or c.startswith(\"Mean\") or\n",
        "                                         c.startswith(\"Std\") or c.startswith(\"Max\") or\n",
        "                                         c.startswith(\"Min\") or c.startswith(\"CV\") or\n",
        "                                         c.startswith(\"MonthAvg\") or c.startswith(\"WeekdayAvg\"))]\n",
        "tree_features_gru = [c for c in tree_features_gru if c not in string_cols_gru]\n",
        "\n",
        "# Scaling\n",
        "sales_scaler_gru = MinMaxScaler()\n",
        "meta_scaler_gru = MinMaxScaler()\n",
        "\n",
        "train_df_gru[\"sales_scaled\"] = sales_scaler_gru.fit_transform(train_df_gru[[\"ì¼ë§¤ì¶œ\"]])\n",
        "train_df_gru[meta_cols_gru] = meta_scaler_gru.fit_transform(train_df_gru[meta_cols_gru])\n",
        "\n",
        "val_df_gru[\"sales_scaled\"] = sales_scaler_gru.transform(val_df_gru[[\"ì¼ë§¤ì¶œ\"]])\n",
        "val_df_gru[meta_cols_gru] = meta_scaler_gru.transform(val_df_gru[meta_cols_gru])\n",
        "\n",
        "# Sequence Dataset\n",
        "LOOKBACK_GRU = 7\n",
        "BATCH_GRU = 64\n",
        "EPOCHS_GRU = 200\n",
        "\n",
        "def create_seq_gru(df, lookback):\n",
        "    X, y = [], []\n",
        "    sales = df[\"sales_scaled\"].values\n",
        "    meta = df[meta_cols_gru].values\n",
        "    for i in range(len(df)-lookback):\n",
        "        seq = np.concatenate([\n",
        "            sales[i:i+lookback].reshape(lookback,1),\n",
        "            meta[i:i+lookback]\n",
        "        ], axis=1)\n",
        "        X.append(seq)\n",
        "        y.append(sales[i+lookback])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train_gru, y_train_gru = create_seq_gru(train_df_gru, LOOKBACK_GRU)\n",
        "X_val_gru, y_val_gru = create_seq_gru(val_df_gru, LOOKBACK_GRU)\n",
        "\n",
        "X_train_gru = torch.tensor(X_train_gru, dtype=torch.float32).to(DEVICE)\n",
        "y_train_gru = torch.tensor(y_train_gru, dtype=torch.float32).to(DEVICE)\n",
        "X_val_gru = torch.tensor(X_val_gru, dtype=torch.float32).to(DEVICE)\n",
        "y_val_gru = torch.tensor(y_val_gru, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Models\n",
        "input_dim_gru = 1 + len(meta_cols_gru)\n",
        "\n",
        "class MetaGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim,\n",
        "            hidden,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.fc1 = nn.Linear(hidden, hidden // 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden // 2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1]\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        return self.fc2(out)\n",
        "\n",
        "gru_model = MetaGRU(input_dim_gru, hidden=64, num_layers=2, dropout=0.3).to(DEVICE)\n",
        "opt_gru = torch.optim.AdamW(gru_model.parameters(), lr=0.0001, weight_decay=0)\n",
        "scheduler_gru = torch.optim.lr_scheduler.ReduceLROnPlateau(opt_gru, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "generator_gru = torch.Generator()\n",
        "generator_gru.manual_seed(SEED)\n",
        "\n",
        "train_dataset_gru = TensorDataset(X_train_gru, y_train_gru)\n",
        "train_loader_gru = DataLoader(train_dataset_gru, batch_size=BATCH_GRU, shuffle=True, generator=generator_gru, worker_init_fn=seed_worker)\n",
        "\n",
        "# Train\n",
        "early_stop_gru = EarlyStopping(patience=15, min_delta=0.0001)\n",
        "print(\"=\" * 50)\n",
        "print(\"GRU í•™ìŠµ ì‹œì‘\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ep in range(EPOCHS_GRU):\n",
        "    gru_model.train()\n",
        "    train_loss_gru = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader_gru:\n",
        "        opt_gru.zero_grad()\n",
        "        pred_g = gru_model(batch_x).squeeze()\n",
        "        loss_g = criterion(pred_g, batch_y)\n",
        "        loss_g.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gru_model.parameters(), max_norm=0.5)\n",
        "        opt_gru.step()\n",
        "        train_loss_gru += loss_g.item()\n",
        "\n",
        "    gru_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred_g = gru_model(X_val_gru).squeeze()\n",
        "        val_loss_gru = criterion(val_pred_g, y_val_gru).item()\n",
        "\n",
        "    scheduler_gru.step(val_loss_gru)\n",
        "\n",
        "    if (ep + 1) % 20 == 0:\n",
        "        print(f\"Epoch {ep+1}/{EPOCHS_GRU} - GRU: {val_loss_gru:.6f}\")\n",
        "\n",
        "    if early_stop_gru(val_loss_gru, gru_model):\n",
        "        print(f\"GRU Early stopping at epoch {ep+1}\")\n",
        "        early_stop_gru.load_best_model(gru_model)\n",
        "        break\n",
        "\n",
        "early_stop_gru.load_best_model(gru_model)\n",
        "print(\"âœ… GRU í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "# PART B: GRU Direct Multi-step ì˜ˆì¸¡ (ë°ì´í„° ëˆ„ìˆ˜ ìˆ˜ì •)\n",
        "train_full_gru = make_features_gru(train.copy())\n",
        "\n",
        "# âœ… trainì˜ í‰ê· ê°’ ë¯¸ë¦¬ ê³„ì‚° (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
        "train_full_gru_for_avg = make_features_gru(train.copy())\n",
        "month_avg_train_gru = train_full_gru_for_avg.groupby(\"Month\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "weekday_avg_train_gru = train_full_gru_for_avg.groupby(\"DayOfWeek\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "\n",
        "train_full_scaled_gru = train_full_gru.copy()\n",
        "train_full_scaled_gru[\"sales_scaled\"] = sales_scaler_gru.transform(train_full_gru[[\"ì¼ë§¤ì¶œ\"]])\n",
        "train_full_scaled_gru[meta_cols_gru] = meta_scaler_gru.transform(train_full_gru[meta_cols_gru])\n",
        "\n",
        "# âœ… ìˆ˜ì •: test ë°ì´í„° ì‚¬ìš© ì•ˆ í•¨, ë¯¸ë˜ ë‚ ì§œë§Œ ìƒì„±\n",
        "future_dates_gru = pd.date_range(start=test[\"ì˜ì—…ì¼ì\"].min(), end=test[\"ì˜ì—…ì¼ì\"].max())\n",
        "future_df_gru = pd.DataFrame({\"ì˜ì—…ì¼ì\": future_dates_gru})\n",
        "future_df_gru = future_df_gru.merge(acad, on=\"ì˜ì—…ì¼ì\", how=\"left\")\n",
        "# í•™ì‚¬ì¼ì • ì»¬ëŸ¼ ì¶”ê°€\n",
        "for col in acad_cols:\n",
        "    if col not in future_df_gru.columns:\n",
        "        future_df_gru[col] = 0\n",
        "\n",
        "# meta_cols_gruì— í•„ìš”í•œ ê¸°ë³¸ ì»¬ëŸ¼ í™•ì¸\n",
        "for col in meta_cols_gru:\n",
        "    if col not in future_df_gru.columns:\n",
        "        future_df_gru[col] = 0\n",
        "\n",
        "def predict_gru_direct_safe(model, train_df, future_df, lookback, month_avg_train, weekday_avg_train):\n",
        "    \"\"\"âœ… ë°ì´í„° ëˆ„ìˆ˜ ì—†ëŠ” GRU Direct Multi-step ì˜ˆì¸¡\"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    # history ì´ˆê¸°í™” (train ë°ì´í„°ë§Œ ì‚¬ìš©)\n",
        "    history_df = train_df.copy()\n",
        "\n",
        "    for i in range(len(future_df)):\n",
        "        row = future_df.iloc[i]\n",
        "\n",
        "        # í•„ìš”í•œ ê²½ìš°ì—ë§Œ feature ìƒì„± (train í‰ê· ê°’ ì‚¬ìš©)\n",
        "        if i < lookback:\n",
        "            hist_for_gru = history_df.tail(lookback - i - 1).copy()\n",
        "            current_row = pd.DataFrame([{\n",
        "                \"ì˜ì—…ì¼ì\": row[\"ì˜ì—…ì¼ì\"],\n",
        "                \"ì¼ë§¤ì¶œ\": history_df[\"ì¼ë§¤ì¶œ\"].tail(7).mean() if len(history_df) >= 7 else history_df[\"ì¼ë§¤ì¶œ\"].mean(),\n",
        "                **{col: row[col] if col in row.index else 0 for col in meta_cols_gru}\n",
        "            }])\n",
        "            seq_df_gru = pd.concat([hist_for_gru, current_row], ignore_index=True)\n",
        "        else:\n",
        "            hist_for_gru = history_df.tail(1).copy()\n",
        "            temp_rows = []\n",
        "            for j in range(1, lookback):\n",
        "                if i - j >= 0:\n",
        "                    temp_row = pd.DataFrame([{\n",
        "                        \"ì˜ì—…ì¼ì\": future_df.iloc[i-j][\"ì˜ì—…ì¼ì\"],\n",
        "                        \"ì¼ë§¤ì¶œ\": preds[i-j] if i-j < len(preds) else history_df[\"ì¼ë§¤ì¶œ\"].tail(7).mean(),\n",
        "                        **{col: future_df.iloc[i-j][col] if col in future_df.columns else 0 for col in meta_cols_gru}\n",
        "                    }])\n",
        "                    temp_rows.append(temp_row)\n",
        "\n",
        "            if temp_rows:\n",
        "                seq_df_gru = pd.concat([hist_for_gru] + temp_rows, ignore_index=True)\n",
        "            else:\n",
        "                seq_df_gru = hist_for_gru\n",
        "\n",
        "        # ê¸¸ì´ ë³´ì¥\n",
        "        if len(seq_df_gru) < lookback:\n",
        "            last_row = seq_df_gru.iloc[-1:].copy()\n",
        "            padding_count = lookback - len(seq_df_gru)\n",
        "            padding = pd.concat([last_row] * padding_count, ignore_index=True)\n",
        "            seq_df_gru = pd.concat([seq_df_gru, padding], ignore_index=True)\n",
        "        elif len(seq_df_gru) > lookback:\n",
        "            seq_df_gru = seq_df_gru.tail(lookback).reset_index(drop=True)\n",
        "\n",
        "        # âœ… trainì˜ í‰ê· ê°’ì„ ì‚¬ìš©í•˜ì—¬ feature ìƒì„± (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
        "        seq_df_gru = make_features_gru_safe(seq_df_gru, month_avg_train, weekday_avg_train)\n",
        "        seq_df_gru[\"sales_scaled\"] = sales_scaler_gru.transform(seq_df_gru[[\"ì¼ë§¤ì¶œ\"]].fillna(0))\n",
        "        seq_df_gru[meta_cols_gru] = meta_scaler_gru.transform(seq_df_gru[meta_cols_gru].fillna(0))\n",
        "\n",
        "        # Sequence ìƒì„±\n",
        "        sales_seq_gru = seq_df_gru[\"sales_scaled\"].values.reshape(lookback, 1)\n",
        "        meta_seq_gru = seq_df_gru[meta_cols_gru].values\n",
        "        seq_gru = np.concatenate([sales_seq_gru, meta_seq_gru], axis=1).reshape(1, lookback, -1)\n",
        "\n",
        "        # ì˜ˆì¸¡\n",
        "        with torch.no_grad():\n",
        "            X_gru = torch.tensor(seq_gru, dtype=torch.float32).to(DEVICE)\n",
        "            pred_scaled_gru = model(X_gru).squeeze().cpu().item()\n",
        "            pred_gru = sales_scaler_gru.inverse_transform([[pred_scaled_gru]])[0][0]\n",
        "            pred_gru = max(0, pred_gru)\n",
        "\n",
        "        preds.append(pred_gru)\n",
        "\n",
        "        # history ì—…ë°ì´íŠ¸ (ì˜ˆì¸¡ê°’ ì‚¬ìš©)\n",
        "        new_row = pd.DataFrame([{\n",
        "            \"ì˜ì—…ì¼ì\": row[\"ì˜ì—…ì¼ì\"],\n",
        "            \"ì¼ë§¤ì¶œ\": pred_gru,\n",
        "            **{col: row[col] if col in row.index else 0 for col in meta_cols_gru}\n",
        "        }])\n",
        "        history_df = pd.concat([history_df, new_row], ignore_index=True)\n",
        "\n",
        "    return np.array(preds)\n",
        "\n",
        "future_gru = predict_gru_direct_safe(\n",
        "    gru_model,\n",
        "    train_full_scaled_gru,\n",
        "    future_df_gru,\n",
        "    LOOKBACK_GRU,\n",
        "    month_avg_train_gru,\n",
        "    weekday_avg_train_gru\n",
        ")\n",
        "print(f\"GRU ì˜ˆì¸¡ ì™„ë£Œ - ë²”ìœ„: {future_gru.min():.2f} ~ {future_gru.max():.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# PART 3: ì„¸ ë²ˆì§¸ ì½”ë“œ - Tree ëª¨ë¸ë“¤ (ê·¸ëŒ€ë¡œ ì´ì–´ë¶™ì´ê¸°)\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PART 3: Tree ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ì„¸ ë²ˆì§¸ ì½”ë“œ ê·¸ëŒ€ë¡œ)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ì„¸ ë²ˆì§¸ ì½”ë“œì˜ Feature Engineering\n",
        "def make_features_tree(df):\n",
        "    \"\"\"ì„¸ ë²ˆì§¸ ì½”ë“œ: ì „ì²´ feature ìƒì„±\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"DayOfWeek\"] = df[\"ì˜ì—…ì¼ì\"].dt.dayofweek\n",
        "    df[\"Month\"] = df[\"ì˜ì—…ì¼ì\"].dt.month\n",
        "    df[\"Day\"] = df[\"ì˜ì—…ì¼ì\"].dt.day\n",
        "    df[\"IsWeekend\"] = (df[\"DayOfWeek\"] >= 5).astype(int)\n",
        "    df[\"OpHoursFactor\"] = df[\"DayOfWeek\"].apply(lambda x: 1 if x<5 else (0.55 if x==5 else 0))\n",
        "    df[\"IsZeroSales\"] = (df[\"ì¼ë§¤ì¶œ\"] == 0).astype(int)\n",
        "\n",
        "    for lag in [1,2,3,7,14,28]:\n",
        "        df[f\"Lag{lag}\"] = df[\"ì¼ë§¤ì¶œ\"].shift(lag)\n",
        "    for w in [7,14,28]:\n",
        "        df[f\"Mean{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).mean()\n",
        "        df[f\"Std{w}\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(w).std()\n",
        "\n",
        "    df[\"Mean3\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(3).mean()\n",
        "    df[\"Max7\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(7).max()\n",
        "    df[\"Min7\"] = df[\"ì¼ë§¤ì¶œ\"].rolling(7).min()\n",
        "    df[\"CV7\"] = df[\"Std7\"] / (df[\"Mean7\"] + 1e-8)\n",
        "\n",
        "    if len(df) > 0:\n",
        "        month_avg = df.groupby(\"Month\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "        df[\"MonthAvg\"] = df[\"Month\"].map(month_avg)\n",
        "        weekday_avg = df.groupby(\"DayOfWeek\")[\"ì¼ë§¤ì¶œ\"].mean()\n",
        "        df[\"WeekdayAvg\"] = df[\"DayOfWeek\"].map(weekday_avg)\n",
        "\n",
        "    return df.fillna(0)\n",
        "\n",
        "train_tree = make_features_tree(train.copy())\n",
        "train_df_tree, val_df_tree = train_test_split(train_tree, test_size=0.2, shuffle=False, random_state=SEED)\n",
        "\n",
        "# Feature Selection\n",
        "numeric_cols_tree = train_df_tree.select_dtypes(include=[np.number]).columns.tolist()\n",
        "string_cols_tree = train_df_tree.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "meta_cols_tree = [\n",
        "    c for c in numeric_cols_tree\n",
        "    if c not in [\"ì¼ë§¤ì¶œ\", \"sales_scaled\"]\n",
        "    and not c.startswith(\"Lag\")\n",
        "    and not c.startswith(\"Mean\")\n",
        "    and not c.startswith(\"Std\")\n",
        "    and not c.startswith(\"Max\")\n",
        "    and not c.startswith(\"Min\")\n",
        "    and not c.startswith(\"CV\")\n",
        "    and not c.startswith(\"MonthAvg\")\n",
        "    and not c.startswith(\"WeekdayAvg\")\n",
        "    and c not in acad_cols\n",
        "]\n",
        "meta_cols_tree = [c for c in meta_cols_tree if c not in string_cols_tree]\n",
        "\n",
        "tree_features_tree = meta_cols_tree + [c for c in train_df_tree.columns\n",
        "                                       if (c.startswith(\"Lag\") or c.startswith(\"Mean\") or\n",
        "                                           c.startswith(\"Std\") or c.startswith(\"Max\") or\n",
        "                                           c.startswith(\"Min\") or c.startswith(\"CV\") or\n",
        "                                           c.startswith(\"MonthAvg\") or c.startswith(\"WeekdayAvg\"))]\n",
        "tree_features_tree = [c for c in tree_features_tree if c not in string_cols_tree]\n",
        "\n",
        "# Tree ëª¨ë¸ í•™ìŠµ\n",
        "lgb_params_tree = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 100,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_child_samples': 20,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 2.0,\n",
        "    'max_depth': 10,\n",
        "    'early_stopping_rounds': 50,\n",
        "    'verbose': -1,\n",
        "    'force_col_wise': True,\n",
        "    'random_state': SEED,\n",
        "    'deterministic': True\n",
        "}\n",
        "\n",
        "xgb_params_tree = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'rmse',\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.01,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'min_child_weight': 5,\n",
        "    'gamma': 0.1,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 2.0,\n",
        "    'tree_method': 'hist',\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'random_state': SEED,\n",
        "    'deterministic': True\n",
        "}\n",
        "\n",
        "lgb_train_tree = lgb.Dataset(train_df_tree[tree_features_tree], label=train_df_tree[\"ì¼ë§¤ì¶œ\"])\n",
        "lgb_val_tree = lgb.Dataset(val_df_tree[tree_features_tree], label=val_df_tree[\"ì¼ë§¤ì¶œ\"], reference=lgb_train_tree)\n",
        "\n",
        "print(\"LightGBM í•™ìŠµ ì¤‘...\")\n",
        "lgb_model_tree = lgb.train(\n",
        "    lgb_params_tree,\n",
        "    lgb_train_tree,\n",
        "    valid_sets=[lgb_train_tree, lgb_val_tree],\n",
        "    valid_names=['train', 'val'],\n",
        "    num_boost_round=1000,\n",
        "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
        ")\n",
        "\n",
        "xgb_train_tree = xgb.DMatrix(train_df_tree[tree_features_tree], label=train_df_tree[\"ì¼ë§¤ì¶œ\"])\n",
        "xgb_val_tree = xgb.DMatrix(val_df_tree[tree_features_tree], label=val_df_tree[\"ì¼ë§¤ì¶œ\"])\n",
        "\n",
        "print(\"\\nXGBoost í•™ìŠµ ì¤‘...\")\n",
        "xgb_model_tree = xgb.train(\n",
        "    xgb_params_tree,\n",
        "    xgb_train_tree,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(xgb_train_tree, 'train'), (xgb_val_tree, 'val')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=100\n",
        ")\n",
        "\n",
        "# ì „ì²´ trainìœ¼ë¡œ ì¬í•™ìŠµ\n",
        "train_full_tree = make_features_tree(train.copy())\n",
        "\n",
        "for col in tree_features_tree:\n",
        "    if col not in train_full_tree.columns:\n",
        "        train_full_tree[col] = 0\n",
        "\n",
        "lgb_best_iter_tree = lgb_model_tree.best_iteration if hasattr(lgb_model_tree, 'best_iteration') else 200\n",
        "xgb_best_iter_tree = xgb_model_tree.best_iteration if hasattr(xgb_model_tree, 'best_iteration') and xgb_model_tree.best_iteration is not None else 200\n",
        "\n",
        "print(f\"\\nLightGBM ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘... (iteration: {lgb_best_iter_tree})\")\n",
        "lgb_params_full_tree = lgb_params_tree.copy()\n",
        "lgb_params_full_tree.pop('early_stopping_rounds', None)\n",
        "lgb_train_full_tree = lgb.Dataset(train_full_tree[tree_features_tree], label=train_full_tree[\"ì¼ë§¤ì¶œ\"])\n",
        "lgb_model_full_tree = lgb.train(lgb_params_full_tree, lgb_train_full_tree, num_boost_round=lgb_best_iter_tree)\n",
        "\n",
        "print(f\"\\nXGBoost ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘... (iteration: {xgb_best_iter_tree})\")\n",
        "xgb_train_full_tree = xgb.DMatrix(train_full_tree[tree_features_tree], label=train_full_tree[\"ì¼ë§¤ì¶œ\"])\n",
        "xgb_model_full_tree = xgb.train(xgb_params_tree, xgb_train_full_tree, num_boost_round=xgb_best_iter_tree, verbose_eval=0)\n",
        "\n",
        "# Tree autoregressive ì˜ˆì¸¡\n",
        "def predict_tree_autoreg(model, model_type, history_df, n_steps, xgb_best_iter=None):\n",
        "    \"\"\"ì„¸ ë²ˆì§¸ ì½”ë“œ: Tree ëª¨ë¸ ìë™íšŒê·€ ì˜ˆì¸¡\"\"\"\n",
        "    preds = []\n",
        "    temp_df = history_df.copy().reset_index(drop=True)\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        if len(temp_df) == 0:\n",
        "            preds.append(0)\n",
        "            continue\n",
        "\n",
        "        for feat in tree_features_tree:\n",
        "            if feat not in temp_df.columns:\n",
        "                temp_df[feat] = 0\n",
        "\n",
        "        if len(temp_df) >= 7:\n",
        "            recent_7d_avg = temp_df[\"ì¼ë§¤ì¶œ\"].tail(7).mean()\n",
        "        else:\n",
        "            recent_7d_avg = temp_df[\"ì¼ë§¤ì¶œ\"].mean() if len(temp_df) > 0 else 0\n",
        "\n",
        "        lag1 = temp_df[\"ì¼ë§¤ì¶œ\"].iloc[-1] if len(temp_df) > 0 else 0\n",
        "\n",
        "        try:\n",
        "            if model_type == 'lgb':\n",
        "                last_row = temp_df[tree_features_tree].iloc[-1:].copy()\n",
        "                pred = model.predict(last_row, num_iteration=model.best_iteration if hasattr(model, 'best_iteration') else None)[0]\n",
        "            else:\n",
        "                last_row = temp_df[tree_features_tree].iloc[-1:].copy()\n",
        "                if xgb_best_iter is not None and xgb_best_iter > 0:\n",
        "                    pred = model.predict(xgb.DMatrix(last_row), iteration_range=(0, xgb_best_iter))[0]\n",
        "                else:\n",
        "                    pred = model.predict(xgb.DMatrix(last_row))[0]\n",
        "        except Exception as e:\n",
        "            pred = lag1 if lag1 > 0 else recent_7d_avg\n",
        "\n",
        "        pred = max(0, pred)\n",
        "        raw_pred = pred\n",
        "\n",
        "        # ìŠ¤ì¼€ì¼ ë³´ì •\n",
        "        if pred < recent_7d_avg * 0.15 and recent_7d_avg > 0:\n",
        "            if lag1 > 0:\n",
        "                pred = max(lag1 * 0.4, min(lag1 * 1.3, pred * (lag1 / max(pred, 1))))\n",
        "            else:\n",
        "                pred = max(recent_7d_avg * 0.3, min(recent_7d_avg * 1.1, pred * (recent_7d_avg / max(pred, 1))))\n",
        "\n",
        "        if pred < 10000 and recent_7d_avg > 50000:\n",
        "            pred = max(10000, pred)\n",
        "\n",
        "        preds.append(pred)\n",
        "\n",
        "        if len(temp_df) > 0:\n",
        "            new_row = temp_df.iloc[-1].copy()\n",
        "            new_row[\"ì¼ë§¤ì¶œ\"] = pred\n",
        "            if 'ì˜ì—…ì¼ì' in new_row and pd.notna(new_row['ì˜ì—…ì¼ì']):\n",
        "                new_row['ì˜ì—…ì¼ì'] = new_row['ì˜ì—…ì¼ì'] + pd.Timedelta(days=1)\n",
        "            temp_df = pd.concat([temp_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "            temp_df = make_features_tree(temp_df)\n",
        "            temp_df = temp_df.reset_index(drop=True)\n",
        "\n",
        "    return np.array(preds)\n",
        "\n",
        "print(\"\\nTree ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
        "future_lgb = predict_tree_autoreg(lgb_model_full_tree, 'lgb', train_full_tree, len(test))\n",
        "future_xgb = predict_tree_autoreg(xgb_model_full_tree, 'xgb', train_full_tree, len(test), xgb_best_iter_tree)\n",
        "future_tree = (future_lgb + future_xgb) / 2\n",
        "print(f\"Tree ì˜ˆì¸¡ ì™„ë£Œ - LGB ë²”ìœ„: {future_lgb.min():.2f} ~ {future_lgb.max():.2f}\")\n",
        "print(f\"Tree ì˜ˆì¸¡ ì™„ë£Œ - XGB ë²”ìœ„: {future_xgb.min():.2f} ~ {future_xgb.max():.2f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# ìµœì¢… ì•™ìƒë¸” (Scale ë§ì¶¤)\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ìµœì¢… ì•™ìƒë¸” (Scale ì¡°ì •)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ê° ëª¨ë¸ ì˜ˆì¸¡ê°’ ìŠ¤ì¼€ì¼ í™•ì¸\n",
        "print(f\"\\nê° ëª¨ë¸ ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
        "print(f\"  LSTM - í‰ê· : {future_lstm.mean():.2f}, ìµœì†Œ: {future_lstm.min():.2f}, ìµœëŒ€: {future_lstm.max():.2f}\")\n",
        "print(f\"  GRU  - í‰ê· : {future_gru.mean():.2f}, ìµœì†Œ: {future_gru.min():.2f}, ìµœëŒ€: {future_gru.max():.2f}\")\n",
        "print(f\"  Tree - í‰ê· : {future_tree.mean():.2f}, ìµœì†Œ: {future_tree.min():.2f}, ìµœëŒ€: {future_tree.max():.2f}\")\n",
        "\n",
        "# NN ì•™ìƒë¸”\n",
        "future_nn_ensemble = (future_lstm + future_gru) / 2\n",
        "\n",
        "# ìµœì¢… ì•™ìƒë¸” (0.3*Tree + 0.7*NN)\n",
        "future_final = 0.3 * future_tree + 0.7 * future_nn_ensemble\n",
        "\n",
        "print(f\"\\nìµœì¢… ì•™ìƒë¸” - í‰ê· : {future_final.mean():.2f}, ìµœì†Œ: {future_final.min():.2f}, ìµœëŒ€: {future_final.max():.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# TEST íŒŒì¼ ë¡œë“œ ë° ì„±ëŠ¥ í‰ê°€\n",
        "# ============================================\n",
        "matched = test[test[\"ì¼ë§¤ì¶œ\"].notna()].copy()\n",
        "matched[\"ì¼ë§¤ì¶œ\"] = pd.to_numeric(matched[\"ì¼ë§¤ì¶œ\"], errors='coerce').fillna(0).astype(float)\n",
        "\n",
        "pred_df = pd.DataFrame({\n",
        "    \"ì˜ì—…ì¼ì\": future_dates[:len(matched)],\n",
        "    \"ì˜ˆì¸¡_LSTM\": future_lstm[:len(matched)],\n",
        "    \"ì˜ˆì¸¡_GRU\": future_gru[:len(matched)],\n",
        "    \"ì˜ˆì¸¡_NN_Ensemble\": future_nn_ensemble[:len(matched)],\n",
        "    \"ì˜ˆì¸¡_LGB\": future_lgb[:len(matched)],\n",
        "    \"ì˜ˆì¸¡_XGB\": future_xgb[:len(matched)],\n",
        "    \"Tree_Ensemble\": future_tree[:len(matched)],\n",
        "    \"Final_Ensemble\": future_final[:len(matched)]\n",
        "})\n",
        "\n",
        "result_df = matched.merge(pred_df, on=\"ì˜ì—…ì¼ì\", how=\"inner\")\n",
        "\n",
        "if len(result_df) == 0:\n",
        "    print(\"âš ï¸ ê²½ê³ : ì˜ˆì¸¡ ë‚ ì§œì™€ test ë‚ ì§œê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\")\n",
        "else:\n",
        "    print(f\"\\nâœ… ë‚ ì§œ ë§¤ì¹­ ì™„ë£Œ! (ë§¤ì¹­ëœ ê°œìˆ˜: {len(result_df)})\")\n",
        "\n",
        "# ì„±ëŠ¥ í‰ê°€ í•¨ìˆ˜\n",
        "def MAE(a, b): return np.mean(np.abs(a-b))\n",
        "def RMSE(a, b): return np.sqrt(np.mean((a-b)**2))\n",
        "def SMAPE(a, b): return np.mean(2 * np.abs(a-b) / (np.abs(a)+np.abs(b)+1e-9)) * 100\n",
        "\n",
        "if len(result_df) > 0:\n",
        "    y_true = result_df[\"ì¼ë§¤ì¶œ\"].values\n",
        "    print(\"\\n\" + \"=\"*20 + \" TEST ì„±ëŠ¥ \" + \"=\"*20)\n",
        "\n",
        "    print(\"\\n--- Neural Networks ---\")\n",
        "    print(f\"LSTM MAE   : {MAE(y_true, result_df['ì˜ˆì¸¡_LSTM'].values):.2f}\")\n",
        "    print(f\"LSTM RMSE  : {RMSE(y_true, result_df['ì˜ˆì¸¡_LSTM'].values):.2f}\")\n",
        "    print(f\"LSTM SMAPE : {SMAPE(y_true, result_df['ì˜ˆì¸¡_LSTM'].values):.2f}%\\n\")\n",
        "\n",
        "    print(f\"GRU  MAE   : {MAE(y_true, result_df['ì˜ˆì¸¡_GRU'].values):.2f}\")\n",
        "    print(f\"GRU  RMSE  : {RMSE(y_true, result_df['ì˜ˆì¸¡_GRU'].values):.2f}\")\n",
        "    print(f\"GRU  SMAPE : {SMAPE(y_true, result_df['ì˜ˆì¸¡_GRU'].values):.2f}%\\n\")\n",
        "\n",
        "    print(\"--- NN Ensemble (LSTM + GRU) ---\")\n",
        "    print(f\"NN_Ensemble MAE   : {MAE(y_true, result_df['ì˜ˆì¸¡_NN_Ensemble'].values):.2f}\")\n",
        "    print(f\"NN_Ensemble RMSE  : {RMSE(y_true, result_df['ì˜ˆì¸¡_NN_Ensemble'].values):.2f}\")\n",
        "    print(f\"NN_Ensemble SMAPE : {SMAPE(y_true, result_df['ì˜ˆì¸¡_NN_Ensemble'].values):.2f}%\\n\")\n",
        "\n",
        "    print(\"\\n--- Tree Models ---\")\n",
        "    print(f\"LGB  MAE   : {MAE(y_true, result_df['ì˜ˆì¸¡_LGB'].values):.2f}\")\n",
        "    print(f\"LGB  RMSE  : {RMSE(y_true, result_df['ì˜ˆì¸¡_LGB'].values):.2f}\")\n",
        "    print(f\"LGB  SMAPE : {SMAPE(y_true, result_df['ì˜ˆì¸¡_LGB'].values):.2f}%\\n\")\n",
        "\n",
        "    print(f\"XGB  MAE   : {MAE(y_true, result_df['ì˜ˆì¸¡_XGB'].values):.2f}\")\n",
        "    print(f\"XGB  RMSE  : {RMSE(y_true, result_df['ì˜ˆì¸¡_XGB'].values):.2f}\")\n",
        "    print(f\"XGB  SMAPE : {SMAPE(y_true, result_df['ì˜ˆì¸¡_XGB'].values):.2f}%\\n\")\n",
        "\n",
        "    print(\"\\n--- Tree Ensemble ---\")\n",
        "    print(f\"Tree MAE   : {MAE(y_true, result_df['Tree_Ensemble'].values):.2f}\")\n",
        "    print(f\"Tree RMSE  : {RMSE(y_true, result_df['Tree_Ensemble'].values):.2f}\")\n",
        "    print(f\"Tree SMAPE : {SMAPE(y_true, result_df['Tree_Ensemble'].values):.2f}%\\n\")\n",
        "\n",
        "    print(\"\\n--- Final Ensemble (0.3*Tree + 0.7*NN_Ensemble) ---\")\n",
        "    print(f\"MAE   : {MAE(y_true, result_df['Final_Ensemble'].values):.2f}\")\n",
        "    print(f\"RMSE  : {RMSE(y_true, result_df['Final_Ensemble'].values):.2f}\")\n",
        "    print(f\"SMAPE : {SMAPE(y_true, result_df['Final_Ensemble'].values):.2f}%\")\n",
        "    print(\"=\"*52)\n",
        "\n",
        "# CSV ì €ì¥\n",
        "output = pd.DataFrame({\n",
        "    \"ì˜ì—…ì¼ì\": result_df[\"ì˜ì—…ì¼ì\"].values if len(result_df) > 0 else test[\"ì˜ì—…ì¼ì\"].values,\n",
        "    \"ì¼ë§¤ì¶œ\": result_df[\"ì¼ë§¤ì¶œ\"].values if len(result_df) > 0 else matched[\"ì¼ë§¤ì¶œ\"].values,\n",
        "    \"ì˜ˆì¸¡_LSTM\": future_lstm[:len(test)],\n",
        "    \"ì˜ˆì¸¡_GRU\": future_gru[:len(test)],\n",
        "    \"ì˜ˆì¸¡_NN_Ensemble\": future_nn_ensemble[:len(test)],\n",
        "    \"ì˜ˆì¸¡_LGB\": future_lgb[:len(test)],\n",
        "    \"ì˜ˆì¸¡_XGB\": future_xgb[:len(test)],\n",
        "    \"Tree_Ensemble\": future_tree[:len(test)],\n",
        "    \"Final_Ensemble\": future_final[:len(test)]\n",
        "})\n",
        "\n",
        "output.to_csv(\"final_test_prediction_optimal.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"\\nğŸ“Œ ì €ì¥ ì™„ë£Œ â†’ final_test_prediction_optimal.csv\")\n",
        "print(output.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QgYiGZzHKOC",
        "outputId": "b76760cd-8d06-453d-f31a-eb6170eb6205"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ë°ì´í„° ë¡œë“œ ì¤‘...\n",
            "============================================================\n",
            "\n",
            "í•™ì‚¬ì¼ì • ë°ì´í„° ì»¬ëŸ¼: ['weekday', 'weekend', 'holiday', 'semester', 'seasonal', 'exam', 'ceremony', 'dormitory']\n",
            "\n",
            "============================================================\n",
            "PART 1: LSTM ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ì²« ë²ˆì§¸ ì½”ë“œ ê·¸ëŒ€ë¡œ)\n",
            "============================================================\n",
            "==================================================\n",
            "LSTM í•™ìŠµ ì‹œì‘\n",
            "==================================================\n",
            "[Epoch 1/100] LSTM - Val: 0.024348\n",
            "[Epoch 10/100] LSTM - Val: 0.009573\n",
            "[Epoch 20/100] LSTM - Val: 0.016958\n",
            "[Epoch 30/100] LSTM - Val: 0.006880\n",
            "[Epoch 40/100] LSTM - Val: 0.010657\n",
            "[Epoch 50/100] LSTM - Val: 0.007181\n",
            "[Epoch 60/100] LSTM - Val: 0.006977\n",
            "\n",
            "âœ… LSTM Early Stopping at epoch 66!\n",
            "âœ… LSTM í•™ìŠµ ì™„ë£Œ!\n",
            "LSTM ì˜ˆì¸¡ ì™„ë£Œ - ë²”ìœ„: 5321.58 ~ 451870.89\n",
            "\n",
            "============================================================\n",
            "PART 2: GRU ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ë°ì´í„° ëˆ„ìˆ˜ ìˆ˜ì •)\n",
            "============================================================\n",
            "==================================================\n",
            "GRU í•™ìŠµ ì‹œì‘\n",
            "==================================================\n",
            "Epoch 20/200 - GRU: 0.026271\n",
            "Epoch 40/200 - GRU: 0.024101\n",
            "Epoch 60/200 - GRU: 0.019360\n",
            "Epoch 80/200 - GRU: 0.012692\n",
            "Epoch 100/200 - GRU: 0.010577\n",
            "Epoch 120/200 - GRU: 0.009785\n",
            "Epoch 140/200 - GRU: 0.009750\n",
            "GRU Early stopping at epoch 145\n",
            "âœ… GRU í•™ìŠµ ì™„ë£Œ!\n",
            "GRU ì˜ˆì¸¡ ì™„ë£Œ - ë²”ìœ„: 0.00 ~ 173461.38\n",
            "\n",
            "============================================================\n",
            "PART 3: Tree ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ (ì„¸ ë²ˆì§¸ ì½”ë“œ ê·¸ëŒ€ë¡œ)\n",
            "============================================================\n",
            "LightGBM í•™ìŠµ ì¤‘...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttrain's rmse: 80308.3\tval's rmse: 65973.6\n",
            "[200]\ttrain's rmse: 38595.9\tval's rmse: 29993.9\n",
            "[300]\ttrain's rmse: 25009.6\tval's rmse: 16943.2\n",
            "[400]\ttrain's rmse: 20588.9\tval's rmse: 12177.1\n",
            "[500]\ttrain's rmse: 18972.2\tval's rmse: 10346.2\n",
            "[600]\ttrain's rmse: 17809.1\tval's rmse: 9629.69\n",
            "[700]\ttrain's rmse: 16861.7\tval's rmse: 9049.46\n",
            "[800]\ttrain's rmse: 16209.9\tval's rmse: 8842.08\n",
            "[900]\ttrain's rmse: 15506\tval's rmse: 8658.9\n",
            "[1000]\ttrain's rmse: 14951.3\tval's rmse: 8592.05\n",
            "\n",
            "XGBoost í•™ìŠµ ì¤‘...\n",
            "[0]\ttrain-rmse:194784.50601\tval-rmse:162721.36656\n",
            "[100]\ttrain-rmse:80516.32913\tval-rmse:66367.58989\n",
            "[200]\ttrain-rmse:36818.95002\tval-rmse:31159.65709\n",
            "[300]\ttrain-rmse:20428.05023\tval-rmse:18624.77699\n",
            "[400]\ttrain-rmse:14635.31382\tval-rmse:14930.70094\n",
            "[500]\ttrain-rmse:12173.77042\tval-rmse:13434.98809\n",
            "[600]\ttrain-rmse:10807.75790\tval-rmse:12551.09680\n",
            "[700]\ttrain-rmse:9877.26502\tval-rmse:12175.28232\n",
            "[800]\ttrain-rmse:9131.82508\tval-rmse:11931.93612\n",
            "[900]\ttrain-rmse:8470.33418\tval-rmse:11746.90450\n",
            "[999]\ttrain-rmse:7957.40841\tval-rmse:11541.48982\n",
            "\n",
            "LightGBM ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘... (iteration: 961)\n",
            "\n",
            "XGBoost ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘... (iteration: 999)\n",
            "\n",
            "Tree ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\n",
            "Tree ì˜ˆì¸¡ ì™„ë£Œ - LGB ë²”ìœ„: 3787.76 ~ 22697.14\n",
            "Tree ì˜ˆì¸¡ ì™„ë£Œ - XGB ë²”ìœ„: 9897.95 ~ 75657.14\n",
            "\n",
            "============================================================\n",
            "ìµœì¢… ì•™ìƒë¸” (Scale ì¡°ì •)\n",
            "============================================================\n",
            "\n",
            "ê° ëª¨ë¸ ì˜ˆì¸¡ê°’ í†µê³„:\n",
            "  LSTM - í‰ê· : 268235.28, ìµœì†Œ: 5321.58, ìµœëŒ€: 451870.89\n",
            "  GRU  - í‰ê· : 38903.34, ìµœì†Œ: 0.00, ìµœëŒ€: 173461.38\n",
            "  Tree - í‰ê· : 9257.19, ìµœì†Œ: 6856.46, ìµœëŒ€: 49177.14\n",
            "\n",
            "ìµœì¢… ì•™ìƒë¸” - í‰ê· : 110275.67, ìµœì†Œ: 26953.38, ìµœëŒ€: 174206.41\n",
            "\n",
            "âœ… ë‚ ì§œ ë§¤ì¹­ ì™„ë£Œ! (ë§¤ì¹­ëœ ê°œìˆ˜: 83)\n",
            "\n",
            "==================== TEST ì„±ëŠ¥ ====================\n",
            "\n",
            "--- Neural Networks ---\n",
            "LSTM MAE   : 120238.97\n",
            "LSTM RMSE  : 149805.10\n",
            "LSTM SMAPE : 91.58%\n",
            "\n",
            "GRU  MAE   : 156880.56\n",
            "GRU  RMSE  : 192372.50\n",
            "GRU  SMAPE : 154.66%\n",
            "\n",
            "--- NN Ensemble (LSTM + GRU) ---\n",
            "NN_Ensemble MAE   : 100844.03\n",
            "NN_Ensemble RMSE  : 116136.21\n",
            "NN_Ensemble SMAPE : 98.21%\n",
            "\n",
            "\n",
            "--- Tree Models ---\n",
            "LGB  MAE   : 161622.41\n",
            "LGB  RMSE  : 213477.47\n",
            "LGB  SMAPE : 193.33%\n",
            "\n",
            "XGB  MAE   : 160868.06\n",
            "XGB  RMSE  : 208555.71\n",
            "XGB  SMAPE : 186.55%\n",
            "\n",
            "\n",
            "--- Tree Ensemble ---\n",
            "Tree MAE   : 161245.24\n",
            "Tree RMSE  : 210958.52\n",
            "Tree SMAPE : 189.84%\n",
            "\n",
            "\n",
            "--- Final Ensemble (0.3*Tree + 0.7*NN_Ensemble) ---\n",
            "MAE   : 116294.65\n",
            "RMSE  : 133004.72\n",
            "SMAPE : 114.63%\n",
            "====================================================\n",
            "\n",
            "ğŸ“Œ ì €ì¥ ì™„ë£Œ â†’ final_test_prediction_optimal.csv\n",
            "        ì˜ì—…ì¼ì       ì¼ë§¤ì¶œ        ì˜ˆì¸¡_LSTM         ì˜ˆì¸¡_GRU  ì˜ˆì¸¡_NN_Ensemble  \\\n",
            "0 2025-08-10       0.0    5321.580369   57820.419069    31570.999719   \n",
            "1 2025-08-11       0.0   53123.510890  150116.431104   101619.970997   \n",
            "2 2025-08-12   81300.0  109297.696522   51706.643821    80502.170172   \n",
            "3 2025-08-13  114900.0  147598.623477       0.000000    73799.311738   \n",
            "4 2025-08-14       0.0  168051.428412       0.000000    84025.714206   \n",
            "5 2025-08-15       0.0  173473.923080       0.000000    86736.961540   \n",
            "6 2025-08-16       0.0  141943.866098  173461.377262   157702.621680   \n",
            "7 2025-08-17       0.0  138977.950662   71845.112984   105411.531823   \n",
            "8 2025-08-18       0.0  172864.025241  100173.285100   136518.655171   \n",
            "9 2025-08-19       0.0  185063.947923  100225.929514   142644.938719   \n",
            "\n",
            "         ì˜ˆì¸¡_LGB        ì˜ˆì¸¡_XGB  Tree_Ensemble  Final_Ensemble  \n",
            "0  22697.142857  75657.142857   49177.142857    36852.842660  \n",
            "1  22697.142857  75657.142857   49177.142857    85887.122555  \n",
            "2  22697.142857  11390.265625   17043.704241    61464.630392  \n",
            "3  22697.142857  12631.421875   17664.282366    56958.802927  \n",
            "4  22697.142857  11089.796875   16893.469866    63886.040904  \n",
            "5   3966.227095  10264.156250    7115.191673    62850.430580  \n",
            "6   3787.755376  10438.937500    7113.346438   112525.839107  \n",
            "7   3949.490378  10148.765625    7049.128001    75902.810677  \n",
            "8   3814.974962   9897.953125    6856.464044    97619.997832  \n",
            "9   4682.088111  11225.031250    7953.559680   102237.525007  \n"
          ]
        }
      ]
    }
  ]
}